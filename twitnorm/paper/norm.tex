%% This is file `elsarticle-template-1-num.tex',
%%
%% Copyright 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%%
%% $Id: elsarticle-template-1-num.tex 149 2009-10-08 05:01:15Z rishi $
%% $URL: http://lenova.river-valley.com/svn/elsbst/trunk/elsarticle-template-1-num.tex $
%%
\documentclass[preprint,review,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
\usepackage{graphics}
%% or use the graphicx package for more complicated commands
\usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
\usepackage{epsfig}
\usepackage{epstopdf}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}
\usepackage{color}
\usepackage{caption}
\usepackage{url}
\newcommand\fxbox[1]{\center{\framebox[1.1\width][c]{\strut#1}}}
\newcommand\mxbox[1]{\center{\makebox[1.1\width][c]{\strut#1}}}


\journal{Knowledge Based Systems}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\title{A Graph Based Approach to Text Normalization}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{}

\address{}

\begin{abstract}
%% Text of abstract

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Text Normalization \sep Twitter \sep microblogs \sep social media
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text

\section{Introduction}
\label{sec:introduction}

Social text has become an enormous part of our lives. We are moving towards to an era that we will be talking using machines more than we talk to each other. Social platforms make mass amounts of people communicate via typed or transcribed text. That is the era that the news are spreading digitally via social media other than edited newspaper articles.

This has been also starting of a new era for text analytics researches. The recent studies on social media such that Stock Prediction\cite{DBLP:conf/acl/SiMLLLD13}, politeness detection\cite{DBLP:conf/acl/Danescu-Niculescu-MizilSJLP13}, disaster detection\cite{Sakaki:2010:EST:1772690.1772777} tries to lighten up the road of this digitized future of ours.

Therefore analyzing social media text is a challenge for itself. Due to its noisy nature, many NLP tools are performing poorly on social media text\cite{ritter2010unsupervised}. The problems that noise in the social media text generates for NLP tools can be overcome by some preprocessing steps.

Unlike spoken and written language, digitized language has its own form and nature. Since the beginning of World Wide Web, internet has it's own slang. \textit{lol} meaning \textit{laughing out loudly},  \textit{xoxo} meaning \textit{kissing}, \textit{4u} meaning \textit{for you} are the oldest examples of this slang. Everyday new slangs as well as new words such as iTunes and new abbreviations are coming up. It is a huge, an evolving language that has long gone behind the reach and control of spellcheckers and slang dictionaries.
\begin{table}[tbhp]
\begin{minipage}[c]{.5\linewidth}
\centering
\begin{tabular}[h]{|r|r|}
\hline
ppl & people \\
havent & haven't \\
tmr &   tomorrow \\
soooo &  so \\
sooon &  soon \\
raight & right \\
raight & alright \\
\hline
\end{tabular}
\end{minipage}
\begin{minipage}[c]{.5\linewidth}
\centering
\begin{tabular}[h]{|r|r|}
\hline
r  &  are \\
mor &    more \\
doin &   doing \\
n &      and \\
friiied &  fried \\
finge &  finger \\
kissin & kissing \\
\hline
\end{tabular}
\end{minipage}
\caption{Example of noisy tokens and their normalized form}
\label{tab:normalizations}
\end{table}

Text normalization is a preprocessing step to restore noisy forms of text to its original(canonical) form\cite{Han:2011:LNS:2002472.2002520} to make use of NLP applications or more broadly to understand the digitized text better. For example \textit{talk 2 u later} can be normalized as \textit{talk to you later} or similarly \textit{enormoooos, enrmss, enourmos} cand be normalized as \textit{enormous}. Those noisy tokens are referred as Out of Vocabulary(OOV) words. Normalization task is restoring OOV words to their In Vocabulary(IV) form.

However not every OOV word should be considered for normalization. The social text is continously evolving with new words and named entities that are not in the vocabularies of the systems~\cite{DBLP:conf/acl/HassanM13}. The OOV tokens that should be considered for normalization is referred as ill-formed words. Oppositely an OOV word can sometimes lexically fit an IV word (ex: \textit{tanks} is both an IV word and OOV word with the canonical form \textit{thanks}). The task of recognizing which tokens are OOV, and which of those are ill-formed are beyond the scope of this paper.

Unlike the clean text, noisy text is difficult to model using standard language models. Due to noisy nature of the OOV tokens versions of a text includes multiple different OOV tokens (ex: Table~\ref{tab:ngrams}). It is even more difficult to reach the correct version of the phrase when there is more than one OOV word in the text. A high performance normalization system should been capable of following the information a noisy token includes as good as in modeling well formed tokens.
\begin{table}[tbhp]
\begin{centering}
\begin{tabular}[h]{l}
\hline
with a beautiful smile \\
\hline
with a \textbf{beatiful} smile \\
\textbf{w} a beautiful smile \\
\textbf{wit} a beautiful \textbf{smil} \\
\textbf{wth} a \textbf{btfl} \textbf{sml} \\
\textbf{w} a \textbf{btfl smle} \\
\hline
\end{tabular}
\par\end{centering}
\caption{Example noisy n-grams}
\label{tab:ngrams}
\end{table}

In this paper we propose a new approach to text normalization. A graph based model which can benefit from both lexical, contextual and grammatical features of social text.

\section{Method}
\label{sec:graph}

%Unsupervised and graph based context aware model:

%In our model we made use of both lexical, contextual and shallow properties of the noisy text which makes use of a weighted token co-occurrence graph.

In this paper, we propose a graph based approach that models both contextual similarity features and lexical similarity features among an OOV word to be a normalized and the candidate IV words. A high level overview of our system is shown in Figure~\ref{fig:overview}. An input text is first preprocessed by tokenizing and Part-Of-Speech (POS) tagging. If the text contains an OOV word, the normalization candidates are chosen by making use of the contextual features which are extracted from a pre-generated word-relatedness graph, as well as lexical similarity features. Lexical similarity features are based on edit distance, longest common subsequence ratio, and double metaphone distance. In addition, a slang dictionary is used as an external resource to enrich the normalization candidate set. The details of the approach are explained in the following sub-sections.

%%% FIGURE
\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.6]{fig/overview}
\caption{High level overview of our system}
\label{fig:overview}
\end{center}
\end{figure}

\subsection{Preprocessing}

Tokenization is the first step in our system. Tokenization is the process of breaking the text into words, numbers, symbols, emoticons or in other words the smallest meaningful elements within/of the text called tokens. After tokenization, next in the pipeline is POS tagging each token using a social media pos tagger. Unlike the normal pos taggers social media pos taggers~\cite{DBLP:conf/naacl/OwoputiODGSS13}\cite{Gimpel:2011:PTT:2002736.2002747} provide a broader set of tags that is special to the social text. By this extended set of tags we can identify tokens such as discourse markers~(rt for retweets, cont. for a tweet whose content follows up in the coming tweet) or urls and we can process those tokens within their context.

As in Table~\ref{tab:postags}, after preprocessing, each token has a POS tag with a confidence measure. Later, we make use of these confidence scores in calculating the weight of edges in our context graph.

\begin{table}[htbp]
\begin{minipage}{.5\linewidth}
\begin{tabular}[h]{|llr|}
 \hline
Token & POS tag & Accuracy \\
 \hline
with & P & 0.9963 \\
 \hline
a & D & 0.998 \\
 \hline
beautiful & A & 0.9971 \\
 \hline
smile & G & 0.9712 \\
 \hline
\end{tabular}
\end{minipage}
\begin{minipage}{.5\linewidth}
\begin{tabular}[h]{|llr|}
 \hline
Token & POS tag & Accuracy \\
 \hline
w & P & 0.7486 \\
 \hline
a & D & 0.9920 \\
 \hline
beatiful & A & 0.9733 \\
 \hline
smile & N & 0.9806 \\
 \hline
\end{tabular}
\end{minipage}
\caption{POS tagger output of samples}
\label{tab:postags}
\end{table}

\subsection{Graph construction}

The graph is build using a big dataset of social media text. After preprocessing, we traverse each entry in the dataset and extract nodes and edges.

We define a node with four properties \textit{id, oov, freq, tag}. The token itself plus it's POS tag forms the \textit{id} field. \textit{freq} property indicates the node's frequency count in the dataset. \textit{oov} field is set to True if the token is a OOV word. Following Han et al.\ we used GNU Aspell dictionary (v0.60.6) to determine whether a word is OOV or not.

In the word-relatedness graph, each node is a unique set of a token and a POS tag (see Table~\ref{tab:graph}). This helps us to identify the tokens not only by lexically and contextually but also (in terms of POS tags) gramatically.

\begin{table}[tbhp]
\begin{minipage}[c]{\linewidth}
\fxbox{
Let's$_{\textcolor{red}{L}}$ start$_{\textcolor{red}V}$ this$_{\textcolor{red}D}$ morning$_{\textcolor{red}N}$ w$_{\textcolor{red}P}$ a$_{\textcolor{red}D}$ beatiful$_{\textcolor{red}A}$ smile$_{\textcolor{red}N}$.$_{\textcolor{red},}$
 }\par
\vspace{5mm}
\end{minipage}
\begin{minipage}[c]{\linewidth}
\begin{tabular}[h]{l|l}
Tokens & Let's, start, this, morning, w, a, beatiful, smile,~. \\
\hline
Nodes & Let's$|$L, start$|$V, this$|$D, morning$|$N, w$|$P, a$|$D, beatiful$|$A, smile$|$V, .$|$, \\
\hline
Edges & \{Let's$|$L, start$|$V , distance:1\},\{Let's$|$L, this$|$D, distance:2\}, \\
& ... \\
& \{a$|$D, beatiful$|$A, distance:1\}, \{a$|$D, smile$|$V, distance:2\}, \\
& \{beatiful$|$A, smile$|$V, distance:1\} \\
\end{tabular}
\end{minipage}
\caption{Sample sentence with POS tags, Tokens, The Word-relatedness Graph Nodes and Edges}
\label{tab:graph}
\end{table}

For example if the token \textit{smile} has been seen frequently as a Noun and a Verb and not in other forms in the dataset~(Ex: Table \ref{tab:nodes}), this means that it is not a good candidate for a Pronoun OOV token(that is a Pronoun). On conversely, if lexically and contextually similar enough, \textit{smile} is(can be) a good candidate for a Noun or Verb OOV token.

\begin{table}[hbt]
  \centering
  \begin{tabular}[tc]{l}
    node id : smile$|$A , freq : 3, oov : False, tag : A \\
    node id : smile$|$N , freq : 3403, oov : False, tag : N \\
    node id : smile$|$V , freq : 2796, oov : False, tag : V \\
  \end{tabular}
  \caption{Example nodes including token smile with a frequency greater than 0}
\label{tab:nodes}
\end{table}

Edges are built upon(depending on) the relatedness metrics defined. For two nodes to be classified as related they have to satisfy both two rules:
\begin{itemize}
\item Tokens in an entry are conceptually related if they co-occurs within a word distance of $d_t$.
\item Each node of an edge should have a minimum frequency of $f_t$ in the whole dataset.
\end{itemize}

The edges follow the flow in the entries thus have a direction from the earlier seen token to the coming token. For example the edges in Table~\ref{tab:edges} would be derived from a text including the phrase ``with a beautiful smile''. The \textit{from} property indicates the first word and \textit{to} is the latter in the phrase. Direction and the distance together hold a unique triplet. For each two node with a specific distance there is an edge with a positive weight if that two nodes are related. Each co-occurrence of two related nodes increases the weight of the representing edge with an average of nodes' POS tag confidence score in that specific entry. If we are to expand the graph with our example phrase using the given POS tags and accuracies from Table~\ref{tab:postags}, the increase in the weights would be respectively $0.9963+0.9712/2$, $0.998+0.9712/2$ and $0.9971+0.9712/2$.
\begin{table}[hbt]
  \centering
  \begin{tabular}[tc]{l}
 from : with$|$P, to : smile$|$N, dis : 3, weight : 10.47095 \\
 from : a$|$D, to : smile$|$N, dis : 1, weight : 274.37365 \\
 from : beautiful$|$A, to : smile$|$N, dis : 0, weight : 240.716 \\
\end{tabular}
  \caption{Example edges from sample phrase  ``with a beautiful smile''}
\label{tab:edges}
\end{table}

\input{graph}

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}          ==>>  [#]
%%   \cite[chap. 2]{key} ==>>  [#, chap. 2]
%%   \citet{key}         ==>>  Author [#]

%% References with bibTeX database:
\bibliographystyle{model1-num-names}
\bibliography{sentic}
\end{document}

%%
%% End of file `norm.tex'.
