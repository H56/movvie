\subsection{Graph construction}
Unlike the clean text, noisy text is difficult to model using standard language models. Due to noisy nature of the tokens different versions of a text includes changes in multiple tokens (ex: Table~\ref{tab:ngrams}). It is difficult to reach the correct version of the n-gram when there is more than one OOV word in the text.
\begin{table}[tbhp]
\begin{centering}
\begin{tabular}[h]{l}
\hline
with a beautiful smile \\
\hline
with a \textbf{beatiful} smile \\
\textbf{w} a beautiful smile \\
\textbf{wit} a beautiful \textbf{smil} \\
\textbf{wth} a \textbf{btfl} \textbf{sml} \\
\textbf{w} a \textbf{btfl smle} \\
\hline
\end{tabular}
\par\end{centering}
\caption{Example noisy n-grams}
\label{tab:ngrams}
\end{table}

The word-relatedness graph has tokens with POS tags as nodes and edges are build upon the relatedness metrics we defined. For two token with POS tag to be classified as related we have two rules:
\begin{itemize}
\item Tokens that are in n distance to each other in the text are conceptually related.
\item Tokens should occour in the dataset with the POS tag at least f times.
\end{itemize}


\framebox[1.1\width]{
Let's$_{\textcolor{red}{L}}$ start$_{\textcolor{red}V}$ this$_{\textcolor{red}D}$ morning$_{\textcolor{red}N}$ w$_{\textcolor{red}P}$ a$_{\textcolor{red}D}$ beautiful$_{\textcolor{red}A}$ smle$_{\textcolor{red}N}$.$_{\textcolor{red},}$
} \par

Each node includes a token and tokens' POS tag.

A node consists of four properties \textit{id, ovv, freq, tag}. The token itself plus it's POS tag forms the \textit{id} field. Each token is represented with it's part of speech tag, this helps us to distinguish the different beings of tokens within the texts, helps not to propose smile as a pronoun. \textit{freq} property indicates the node's frequency count in the dataset and OOV field set to True if a token is OOV. Following Han et al. we used GNU Aspell dictionary (v0.60.6) to determine whether a word is OOV.

\begin{verbatim}
 {u'_id': u'smile|A', u'freq': 3, u'oov': False, u'tag': u'A'}
 {u'_id': u'smile|N', u'freq': 3403, u'oov': False, u'tag': u'N'}
 {u'_id': u'smile|V', u'freq': 2796, u'o0v': False, u'tag': u'V'},
\end{verbatim}

The edges graph, represents the context of tweets. It is the co-occurrence information including the distance information between words. For example the edges below would be derived from a text including the phrase ``with a beautiful smile''. The \textit{from} property indicates the first word and \textit{to} is the latter in the co-occurrence. Each co-occurrence of two words increases the weight of the representing edge with the average of their POS tag accuracy score in that specific text. If we are to expand the graph with our example phrase with the given POS tags and accuracies below. The increase in the weights would be respectively $0.9963+0.9712/2$, $0.998+0.9712/2$ and $0.9971+0.9712/2$.

\begin{verbatim}
{ "dis" : 3, "from" : "with|P", "to" : "smile|N", "weight" : 10.47095 }
{ "dis" : 1, "from" : "a|D", "to" : "smile|N", "weight" : 274.37365 }
{ "dis" : 0, "from" : "beautiful|A", "to" : "smile|N", "weight" : 240.716 }
\end{verbatim}



Our model makes use of words up to distance 4 to represent an equal form of a 5-gram language model.


\subsection{Graph Based Contextual Similarity}


Candidate selection from graph

\subsection{Lexical Similarity}

dictionary from graph

slang dictionary lookup

double metaphone 1
edit distance 2


longest common sub-sequence ratio



\subsection{Ranking}

butun parametetreler aynı.Hassan'ların lambdası
