\documentclass[a4paper,onesided,12pt]{report}
\usepackage{styles/fbe_tez}
\usepackage[utf8x]{inputenc} % To use Unicode (e.g. Turkish) characters
\renewcommand{\labelenumi}{(\roman{enumi})}
\usepackage{amsmath, amsthm, amssymb}
 % Some extra symbols
\usepackage[bottom]{footmisc}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{longtable}
\graphicspath{{figures/}} % Graphics will be here

\usepackage{multirow}
%\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
%\pagestyle{empty}
%\includeonly{introduction} % To only process the given file

% Me added
\usepackage{subfig}
\let\belowcaptionskip\abovecaptionskip
\usepackage{color,url,tabularx,amsmath}
\newcommand\fxbox[1]{\center{\framebox[1.1\width][c]{\strut#1}}}
\newcommand\mxbox[1]{\center{\makebox[1.1\width][c]{\strut#1}}}

%%%%%%%%%%%%%%%%%

\newtheorem{thm}{Theorem}[chapter]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
% COVER PAGE
\title{\uppercase{Text Normalization Using Lexical and Contextual Features}}
\turkcebaslik{\uppercase{Kelime ve Bağlam Bilgisi Temelli Metin Normalizasyonu}}
\degree{B.S., Computer Science, Bilgi University, 2007}
\author{Çağıl Uluşahin Sönmez}
\program{Computer Engineering}
\subyear{2014}

% APPROVED BY PAGE
\supervisor{Assist. Prof.~Arzucan Özgür }
%\cosuperi{Title and Name of Cosupervisor I}
%\cosuperii{Title and Name of Cosupervisor II}
\examineri{Prof.~Tunga Güngör}
\examinerii{Assist. Prof.~Gülşen Cebiroğlu Eryiğit}
%\examineriv{}
%\examinerv{}
\dateofapproval{14.01.2014}

\begin{document}
\setlength{\abovedisplayskip}{1cm}
\setlength{\belowdisplayskip}{1cm}

\pagenumbering{roman}
\makemstitle % M.S. thesis
\makeapprovalpage
\begin{acknowledgements}
Acknowledgements come here...
\end{acknowledgements}
\begin{abstract}
The informal nature of social media text, renders it very difficult to be automatically processed by natural language processing tools. Text normalization, which corresponds to restoring the noisy words to their canonical forms, provides a solution to this challenge.
We introduce an unsupervised text normalization approach that utilizes not only lexical, but also contextual and grammatical features of social text.
The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus.
The graph encodes the relative positions of the words with respect to each other, as well as their part-of-speech tags.
The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words, and the double metaphone algorithm to represent the phonetic similarity. Unlike most of the recent approaches that are based on generating normalization dictionaries, the proposed approach performs normalization by considering the context of the noisy words in the input text.
%In other words, a noisy word can be normalized to different canonical forms depending on the context of the word in the input text message.
Our results show that it achieves state-of-the-art F-score performance on a standard data set. In addition, the system can be tuned to achieve very high precision without sacrificing much from recall.
\end{abstract}
\begin{ozet}
Bir sayfa uzunluğunda özet gelecektir.
\end{ozet}
\tableofcontents
\listoffigures
\listoftables
\begin{symbols}
% The title will be typeset as "LIST OF SYMBOLS".
%
% Use a separate \sym command for each symbols definition.
% First Latin symbols in alphabetical order
\sym{$\text{CL}(o_{i})$}{Candidate List of $o_i$}
\sym{$c_{k}$}{$k^{th}$ Candidate Word/Node for the OOV Token in the Given Input Text}
\sym{$\text{EL}(o_i)$}{Edge List of $o_i$}
\sym{$\text{NL}(o_i)$}{Neighbor List of $o_i$}
\sym{$n_{j}$}{$j^{th}$ Neighbour of the OOV Token in the Given Input Text}
\sym{$o_i$}{$i^{th}$ OOV Token in the Given Input Text}
\sym{$t_{distance}$}{Word Distance Threshold for Contextual Association}
\sym{$t_{edit}$}{Edit Distance Threshold}
\sym{$t_{frequency}$}{Word Frequency Threshold}
\sym{$T_i$}{Tag of a Word}
\sym{$t_{phonetic}$}{Phonetic Edit Distance Threshold}
\sym{ }{}
\sym{ }{}
% Then Greek symbols in alphabetical order
\sym{$\lambda$}{Contextual Similarity Minor Score Parameter}
\sym{$\beta$}{Lexical Similarity Minor Score Parameter}

\end{symbols}

\begin{abbreviations}
 % Abbreviations in alphabetical order
\sym{A}{Adjective}
\sym{C}{Punctuation}
\sym{CMU}{Carnegie Mellon University}
\sym{CWA-Graph}{Contextual Word Association Graph}
\sym{D}{Determiner}
\sym{ED}{Edit Distance}
\sym{G}{Miscellaneous words}
\sym{IV}{In Vocabulary}
\sym{L}{Nominal+verbal}
\sym{LCS}{Longest Common Subsequence}
\sym{LCSR}{Longest Common Subsequence Ratio}
\sym{MT}{Machine Translation}
\sym{N}{Noun}
\sym{NLP}{Natural Language Processing}
\sym{OOV}{Out of Vocabulary}
\sym{P}{Preposition}
\sym{POS}{Part of Speech Tag}
\sym{RT}{Retweet}
\sym{SMS}{Short Messaging Service}
\sym{STT}{Speech To Text}
\sym{URL}{Uniform Resource Locator}
\sym{V}{Verb}
\sym{}{}
\sym{}{}
\sym{}{}
\sym{}{}
\end{abbreviations}


\chapter{INTRODUCTION}
\label{chapter:introduction}
\pagenumbering{arabic}
Within the last decade, the common belief among Internet users that social text has~(or should have) its own lexical and grammatical features has naturally given birth to an Internet language and jargon; which has been steadily growing and evolving ever since~\cite{Choudhury:2007:IMS:1326044.1326048, eisenstein2013bad}. This behavioral preference phenomenon brings another challenge of its own. Not only is the Internet jargon itself growing and evolving in an exponential pace, but also since the beginning of the World Wide Web, the Internet has its own slang. \textit{lol} meaning \textit{laughing out loudly},  \textit{xoxo} meaning \textit{kissing}, \textit{4u} meaning \textit{for you} are among the most commonly used examples of this slang. In addition, these specific forms of informal expressions in social text usually take many different lexical forms when generated by each individual, even though the intended contextual meaning might be the same~\cite{eisenstein2013bad}. In other words, with each different individual the same content is being expressed~(written) in different ways. Due to this unpredicted variety of such expressions, it would be appropriate to call this divergency ``noise'' in social text.

The scope of the problem does not end there. In addition, within the last few years, by the increasing use of mobile devices, social text has now been preferred to be transcribed by using Speech-to-Text~(STT) tools. This text input preference is getting trendier and being used more frequently. The insufficient accuracy of such STT tools brings considerable amount of ``additional noise'' to social text. Tools such as spell checkers and slang dictionaries have been shown to be insufficient to cope with this challenge long time ago~\cite{sproat2001normalization}.

Lastly, when we also consider the usual scarcity of attention when people post messages on social media platforms, the problem of analyzing social text actually goes beyond the reach of human cognitive capacity. The mass usage of such social media platforms makes it impossible to derive analysis results in a limited time scope when processed manually. In addition, most automatic Natural Language Processing~(NLP) tools such as named entity recognizers and  dependency parsers generally perform poorly on social media text~\cite{ritter2010unsupervised}.

Text normalization is a preprocessing step to restore noisy words in text to their original~(canonical) forms~\cite{Han:2011:LNS:2002472.2002520} to make use in NLP applications or more broadly to understand the digitized text better. For example, \textit{talk 2 u later} can be normalized as \textit{talk to you later} or similarly \textit{enormoooos, enrmss} and \textit{enourmos} can be normalized as \textit{enormous}. You can find more examples of normalized text in Table~\ref{tab:sentences}. These noisy tokens are referred as Out of Vocabulary~(OOV) words. The normalization task restores the OOV words to their In Vocabulary~(IV) forms. Table~\ref{tab:normalizations} shows sample OOV words encountered in social media text and their corresponding IV forms.


\begin{table}[hbp]
\caption[Sample tweets and their normalized forms.]{Sample tweets and their normalized forms.}
\label{tab:sentences}
\begin{tabular}{|>{\itshape}p{7cm}|p{7cm}|}
\hline
\underline{Its} a beautiful \underline{nite}, \underline{lukin} for \underline{smth} fun to do, I think I \underline{wanna} be \underline{w} \underline{ma} \underline{frnds}. &
\underline{It’s} a beautiful \underline{night}, \underline{looking} for \underline{something} fun to do, I think I \underline{want to} be \underline{with} \underline{my} \underline{friends}. \\
\hline
\underline{Dnt} always follow \underline{da} crowd, stand \underline{4} \underline{wat} \underline{u} \underline{blv} in. &
\underline{Don't} always follow \underline{the} crowd, stand \underline{for} \underline{what} \underline{you} \underline{beleive} in. \\
\hline
@Cloudy me \underline{tht} go be sad \underline{wen} the hangover hold me \underline{tmr}! &

@Cloudy me \underline{that} going to be sad \underline{when} the hangover hold me \underline{tomorrow}! \\
\hline
I \underline{srsly} need some legend of korra \underline{raight} \underline{nao} \#linplz &
I \underline{seriously} need some legend of korra \underline{right} \underline{now} \#linplz \\
\hline
\underline{Wat} was \underline{tht} for \underline{u} \underline{lil} shit, \underline{dnt} \underline{u} draw on my \underline{enlgand} &
\underline{What} was \underline{that} for \underline{you} \underline{little} shit, \underline{don’t} \underline{you} draw on my \underline{England} \\ \hline
Work \underline{f} a \underline{cos}, not for applause. Live life to \underline{exprss}, not to \underline{imprss} :) & Work \underline{for} a \underline{cause}, not for applause. Live life to \underline{express}, not to \underline{impress} :) \\ \hline
\underline{Hav} guts to say \underline{wat} \underline{u} desire.. \underline{Dnt} beat behind \underline{da} bush!! And \underline{1} \underline{mre} \underline{thng} no \underline{mre} say \textbf{\underline{y}} \underline{r} people's man!! &
\underline{Have} guts to say \underline{what} \underline{you} desire.. \underline{Don’t} beat behind \underline{the} bush!! And \underline{one} \underline{more} \underline{thing} no \underline{more} say \textbf{\underline{you}} \underline{are} people's man!! \\
\hline
There \underline{r} \underline{sm} songs \underline{u} don't want \underline{2} listen \underline{2} \underline{yl} walking \underline{cos} when \underline{u} start dancing \underline{ppl} won't \underline{knw} \textbf{\underline{y}}. &
There \underline{are} \underline{some} songs \underline{you} don't want \underline{to} listen \underline{to} \underline{while} walking \underline{because} when \underline{you} start dancing \underline{people} won't \underline{know} \textbf{\underline{why}}. \\
\hline
\end{tabular}
\end{table}


\begin{table}[tbhp]
\caption{Sample noisy tokens and their normalized forms.}
\centering
\begin{minipage}[c]{.49\linewidth}
\begin{tabular}[h]{|p{3.3cm}|p{3cm}|}
\hline
\textbf{Ill-formed word} & \textbf{Normalization} \\
\hline
ppl & people \\ \hline
tmr &   tomorrow \\ \hline
havent & haven't \\ \hline
soooo &  so \\ \hline
raight & right \\ \hline
raight & alright \\ \hline
cos & because \\ \hline
cos & cause \\ \hline
\end{tabular}
\end{minipage}
\begin{minipage}[c]{.49\linewidth}
\centering
\begin{tabular}[h]{|p{3.3cm}|p{3cm}|}
\hline
\textbf{Ill-formed word} & \textbf{Normalization} \\
\hline
r  &  are \\ \hline
n &      and \\ \hline
mor &    more \\ \hline
doin &   doing \\ \hline
finge &  finger \\ \hline
tnks  & thanks \\ \hline
makeing & making \\ \hline
friiied &  fried \\ \hline
\end{tabular}
\end{minipage}
\label{tab:normalizations}
\end{table}

Every OOV word should not be considered for normalization. Social text is continuously evolving with new words and named entities that are not in the vocabularies of the systems~\cite{DBLP:conf/acl/HassanM13}. For example \textit{iPhone, WikiLeaks or tokenizing} have not taken their places in dictionaries yet, so they are OOV words, but they should not be normalized to any other canonical word. In addition, an OOV word can sometimes lexically fit an IV word~(Ex:~\textit{tanks} is both an IV word and an OOV word with the canonical form \textit{thanks}).

The OOV tokens that should be considered for normalization are referred to as ill-formed words. Ill-formed words can be normalized to different canonical words depending on the context of the text. For example, if we look at last two examples in Table~\ref{tab:sentences}, we see that ``y'' is normalized in the first as ``why'' and as ``you'' in the latter. Another example would be ``cos'', it has two common canonical forms ``cause'' and ``because''.
%The task of recognizing which tokens are OOV, and which of those are ill-formed are beyond the scope of this paper.

In~\cite{Choudhury:2007:IMS:1326044.1326048} Choudhury \textit{et al.}~propose that OOV words observed in noisy text can be classified into two groups, unintentional and intentional errors. The unintentional errors are caused by~(i) pressing of the wrong key,~(ii) pressing of a key more than the desired number of times,~(iii) deletion of a character or~(iv) inadequate knowledge of spelling. As for the intentional errors, they can be categorized into four categories: character deletion(``tlk'' for ``talk'', ``msg'' for ``message'', ``tomoro'' for ``tomorrow'', ``mob'' for ``mobile''), phonetic substitution~(``nite'' for ``night'', ``bk'' for ``back'', ``u'' for ``you'', ``m8'' for ``mate''), abbreviations~(``btw'' for ``by the way'', ``kgp'' for ``Kharagpur'') and non-standard usage~(``wanna'' for ``want to'', ``betta'' for ``better'', ``sumfin'' for ``something'', ``b/c'' for ``because'').

In this thesis, we propose a graph based text normalization method that utilizes both contextual and grammatical features of social text. The contextual information of words is modeled by a word association graph that is created from a large social media text corpus. The graph represents the relative positions of the words in the social media text messages and their Part-of-Speech~(POS) tags. The lexical similarity features among the words are modeled using the longest common subsequence ratio and edit distance that encode the surface similarity and the double metaphone algorithm that encodes the phonetic similarity. The proposed approach is unsupervised, which is an important advantage over supervised systems, given the continuously evolving language in the social media domain. The same OOV word may have different appropriate normalizations depending on the context of the input text message. Recently proposed dictionary-based text normalization systems perform dictionary look-up and always normalize the same OOV word to the same IV word regardless of the context of the input text ~\cite{Han:2011:LNS:2002472.2002520,DBLP:conf/acl/HassanM13}. On the other hand, the proposed approach does not only make use of the general context information in a large corpus of social media text, but it also makes use of the context of the OOV word in the input text message. Thus, an OOV word can be normalized to different IV words depending on the context of the input text. Another strength of the proposed system is that it achieves the state-of-the art precision scores, without sacrificing from recall.

\chapter{RELATED WORK}
\label{chapter:related}

Early work on text normalization mostly made use of the noisy channel model. The first work that had a significant performance improvement over the previous research was by Brill and Moore, 2000~\cite{Brill:2000:IEM:1075218.1075255}. They proposed a novel noisy channel model for spell checking based on string to string edits. Their error model depended on probabilistic modeling of sub-string transformations. Toutanova \textit{et al.}, 2002 improved this approach by extending the error model with phonetic similarities over words~\cite{Toutanova:2002:PMI:1073083.1073109}.

Choudhury \textit{et al.}, 2007 developed a supervised Hidden Markov Model based approach for normalizing Short Message Service~(SMS) Texts~\cite{Choudhury:2007:IMS:1326044.1326048}. Cook and Stevenson, 2009 have extended this model by introducing an unsupervised noisy channel model~\cite{Cook:2009:UMT:1642011.1642021}. Rather than using one generic model for all word formations as in Choudhury \textit{et al.}, 2007, they used a mixture model in which each different word formation type was modeled explicitly. Aw \textit{et al.}, 2006 proposed a phrase-based statistical machine translation~(MT) model for the normalization task~\cite{Aw:2006:PSM:1273073.1273078}. They defined the problem as translating the SMS language to the English language. Pennell and Liu, 2011~\cite{pennell2011character} on the other hand, proposed a character level MT system, that is robust to new abbreviations.

The down side of these methods were:~(1) they did not consider contextual features and~(2) each of them assumed that tokens have unique normalizations. However, that is not the case for the normalization task. The OOV tokens are ambiguous and without contextual information it is not possible to build models that can disambiguate transformations correctly.

Another drawback of supervised models is that they require annotated data, which is not readily available and difficult to create for social media text normalization~\cite{DBLP:conf/emnlp/YangE13}.

More recent approaches handled the text normalization task by building normalization lexicons. Han \textit{et al.}, 2011 developed a two phased model, where they only consider the ill-formed OOV words for normalization~\cite{Han:2011:LNS:2002472.2002520}. First a confusion set is generated using the lexical and phonetic distance features. Later, the candidates in the confusion set are ranked using a mixture of dictionary look up, word similarity based on lexical edit distance, phonemic edit distance, prefix sub-string, suffix sub-string and longest common subsequence~(LCS), as well as context support metrics.

Gouws \textit{et al.}, 2011 on the other hand, proposed an approach that depended highly on user-centric information such as the geological location of the users and the twitter client that the tweet is received from~\cite{Gouws:2011:CBL:2021109.2021113}. Using contextual metrics they modeled the transformation distributions.

Liu \textit{et al.}, 2012 proposed a broad coverage normalization system, which integrates an extended noisy channel model, that is based on enhanced letter transformations, visual priming, string and phonetic similarity~\cite{liu2012broad}. They try to improve the performance of the top $n$ normalization candidates by integrating human perspective modeling.
Yang and Eisenstein, 2013 introduced an unsupervised log linear model for text normalization~\cite{DBLP:conf/emnlp/YangE13}. Their joint statistical approach uses local context based on language modeling and surface similarity. Along with dictionary based models, Yang and Eisenstein's model have obtained a significant improvement on the performance of text normalization systems.

Hassan and Menezes, 2013 generated a normalization equivalence lexicon using Markov random walks on a contextual similarity lattice~\cite{DBLP:conf/acl/HassanM13}. Our approach is different from theirs in several ways. First, our system makes use of the context of the OOV word in the input text, whereas their system is a dictionary-based method that always produces the same normalization to a given OOV word, regardless of its context in the input text. Besides the tokens themselves, we make use of the POS tags in creating the graph as well as the relative positions of the words in the social media text. Hassan and Menezes, 2013 create a bipartite graph, that is relatively more conservative in modeling the context of words. Context of a word is modeled as a window of words of size five. That is, two words to the right of a word and two words to the left of a word constitute the context of a word together. Even if one word is not the same, the context is considered to be different. On the other hand, in our graph, each neighboring token contributes to the context information of a word, which leads to both a higher recall and a higher precision.
%We select the most likely IV candidates for an OOV word by considering the strength of associations between the neighbors of the OOV word in the input text, as well as the tokens %in the graph that are strongly associated with these neighbors in the similar contextual and grammatical context.
%One of the biggest difference between our system and theirs is that they make use of a huge clean vocabulary.

%We believe several models such as the morphophonemic similarity, MT, Maximum Likelihood, etc. has their own limits. Higher performance text normalization systems should make %use of contextual analysis.


\chapter{METHODOLOGY}
\label{chapter:method}

%Unsupervised and graph based context aware model:

%In our model we made use of both lexical, contextual and shallow properties of the noisy text which makes use of a weighted token co-occurrence graph.

In this thesis, we propose a graph based approach that models both contextual and lexical similarity features among an OOV word that requires normalization and candidate IV words. A high level overview of our system is shown in Figure~\ref{fig:overview}. An input text is first preprocessed by tokenizing and Part-Of-Speech~(POS) tagging. If the text contains an OOV word, the normalization candidates are chosen by making use of the contextual features which are extracted from a pre-generated directed word association graph, as well as lexical similarity features. Lexical similarity features are based on edit distance, longest common subsequence ratio, and double metaphone distance. In addition, a slang dictionary is used as an external resource to enrich the normalization candidate set. The details of the approach are explained in the following sub-sections.

%%% FIGURE
\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.6]{fig/overview}
\caption{High level overview of our system.}
\label{fig:overview}
\end{center}
\end{figure}

\section{Preprocessing}

%Tokenization is the first step in our system. Tokenization is the process of breaking the text into words, numbers, symbols, emoticons After tokenization, next in the pipeline is POS tagging each token using a social media POS tagger. Unlike the normal POS taggers, social media POS taggers~\cite{DBLP:conf/naacl/OwoputiODGSS13}\cite{Gimpel:2011:PTT:2002736.2002747} provide a broader set of tags that is special to the social text. By this extended set of tags we can identify tokens such as discourse markers~(rt for retweets, cont. for a tweet whose content follows up in the coming tweet) or URLs. So that we can process those tokens within their context.

Tokenization is the first step in our system. It is the process of breaking the text into tokens, which are the smallest meaningful elements such as numbers, symbols, and emoticons. After tokenization, the next step in the pipeline is Part-of-Speech~(POS) tagging each token using a POS tagger specifically designed for social media text. Unlike the regular POS taggers designed for well-written newswire-like text, social media POS taggers provide a broader set of tags specific to the peculiarities of social text~\cite{owoputi2013improved,Gimpel:2011:PTT:2002736.2002747}. Using this extended set of tags we can identify tokens such as discourse markers~(e.g.~rt for retweets, cont.~for a tweet whose content follows up in the coming tweet) or URLs. This enables us to better model the context of the words in social media text. A sample preprocessed sentence is shown in Table~\ref{tab:postagged}.

\begin{table}[tbhp]
\caption{Sample tokenized, POS tagged sentence~(L: nominal+verbal, V: verb, D: determiner, N: noun, P: Preposition, A: adjective, C: punctuation).}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
Let's$_{\textcolor{red}{L}}$ & start$_{\textcolor{red}V}$ & this$_{\textcolor{red}D}$ & morning$_{\textcolor{red}N}$ & w$_{\textcolor{red}P}$ & a$_{\textcolor{red}D}$ & beatiful$_{\textcolor{red}A}$ & smile$_{\textcolor{red}N}$ & .$_{\textcolor{red}C}$\\
\hline
\end{tabular}
\label{tab:postagged}
\end{table}


As shown in Table~\ref{tab:postags}, after preprocessing, each token is assigned a POS tag with a confidence score between 0 and 1. Later, we use these confidence scores in calculating the edge weights in our context graph. Note that even though the words \emph{w} and\emph{ beatiful} are misspelled, they are tagged correctly by the tagger, with lower confidence scores though.

\begin{table}[htbp]
\caption{Sample POS tagger output obtained by using CMU Ark Tagger~\cite{owoputi2013improved,Gimpel:2011:PTT:2002736.2002747}.}
\begin{minipage}{.5\linewidth}
\begin{tabular}[h]{|l>{\itshape}lr|}
 \hline
Token & POS tag & Tag confidence \\
 \hline
with & Preposition & 0.9963 \\
 \hline
a & Determiner & 0.9980 \\
 \hline
beautiful & Adjective & 0.9971 \\
 \hline
smile & Noun & 0.9712 \\
 \hline
\end{tabular}
\end{minipage}
\begin{minipage}{.5\linewidth}
\begin{tabular}[h]{|l>{\itshape}lr|}
 \hline
Token & POS tag & Tag confidence \\
 \hline
w & Preposition & 0.7486 \\
 \hline
a & Determiner & 0.9920 \\
 \hline
beatiful & Adjective & 0.9733 \\
 \hline
smile & Noun & 0.9806 \\
 \hline
\end{tabular}
\end{minipage}
\label{tab:postags}
\end{table}

\section{Graph construction}

Contextual information of words is modeled through a word association graph created by using a large corpus of social media text. The graph encodes the relative positions of the POS tagged words in the text with respect to each other. After preprocessing, each text message in the corpus is traversed in order to extract the nodes and the edges of the graph.
%The graph~(See Figure~\ref{fig:graph}) is build using a big dataset of social media text. After preprocessing, we traverse each entry in the dataset and extract nodes and edges.
A node is defined with four properties: \textit{id, oov, freq} and \textit{tag}. The token itself is the \textit{id} field. The \textit{freq} property indicates the node's frequency count in the dataset. The \textit{oov} field is set to True if the token is an OOV word. Following the prior work by Han and Baldwin, 2011 we used the GNU Aspell dictionary~(v0.60.6) to determine whether a word is OOV or not~\cite{Han:2011:LNS:2002472.2002520}. We also edited the output of Aspell dictionary to accept letters other than ``a'' and ``i'' as OOV words. A portion of the graph that covers parts of the sample sentence in Table~\ref{tab:postagged} is shown in Figure~\ref{fig:graph}.

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.7]{fig/graph}
\caption{Portion of the word association graph for part of the sample sentence in Table~\ref{tab:postagged}.~(d: distance, w: edge weight).}
\label{fig:graph}
\end{center}
\end{figure}

%We define a node with four properties \textit{id, oov, freq, tag}. The token itself plus it's POS tag forms the \textit{id} field. \textit{freq} property indicates the node's frequency count in the dataset. \textit{oov} field is set to True if the token is a OOV word. Following ~\cite{Han:2011:LNS:2002472.2002520} we used GNU Aspell dictionary~(v0.60.6) to determine whether a word is OOV or not.

%In the word-relatedness graph, each node is a unique set of a token and a POS tag~(see Table~\ref{tab:graph}). This helps us to identify the tokens not only lexically and contextually but also~(in terms of POS tags) grammatically.

In the created word association graph, each node is a unique set of a token and its POS tag. This helps us to identify the candidate IV words for a given OOV word by considering not only lexical and contextual similarity, but also grammatical similarity in terms of POS tags. For example if the token \textit{smile} has been frequently seen as a Noun or a Verb, and not in other forms in the dataset~(e.g.~Table~\ref{tab:nodes}), this provides evidence that it is not a good IV candidate as a normalization for an OOV token that has  been tagged as a Pronoun. On the other hand, \textit{smile} can be a good candidate for a Noun or a Verb OOV token, if it is lexically and contextually similar to it.

\begin{table}[hbt]
  \caption{The different nodes in the word association graph representing the token \textit{smile} tagged with different POS tags.}
  \centering
  \begin{tabular}[tc]{|l|l|l|l|}
    \hline

    \textbf{node id} & \textbf{freq} & \textbf{oov} & \textbf{tag} \\
    \hline
    smile & 3 & False & A \\  \hline
    smile & 3403 & False & N \\ \hline
    smile & 2796 & False & V \\ \hline
  \end{tabular}
\label{tab:nodes}
\end{table}

An edge is created between two nodes in the graph, if the corresponding word pair~(i.e.~token/POS pair) are contextually associated. Two words are considered as contextually associated if they satisfy the following criteria:

\begin{itemize}
\item The two words co-occur within a maximum word distance of $t_{distance}$ in a text message in the corpus.
\item Each word has a minimum frequency of $t_{frequency}$ in the corpus.
\end{itemize}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.7]{fig/directionality}
\end{center}
\caption{The directionality of the edges is based on the sequence of words in the text messages in the corpus.}
\vskip\baselineskip % Leave a vertical skip below the figure
\label{fig:direction}
\end{figure}

The directionality of the edges is based  on the sequence of words in the text messages in the corpus~(see Figure~\ref{fig:direction}). In other words, an edge between two nodes is directed from the earlier seen token towards the later seen token. For example, Table~\ref{tab:edges} and Figure~\ref{fig:edges} show the edges that would be derived from a text including the phrase ``with a beautiful smile''. The \textit{from} property indicates the first word and \textit{to} is the latter in the phrase. The direction and the distance together represent a unique triplet. For each pair of nodes with a specific distance there is an edge with a positive weight, if the two nodes are related. Each co-occurrence of two related nodes increases the weight of the edge between them with an average of the nodes' POS tag confidence scores in the text message considered. If we are to expand the graph with the example phrase shown in Table~\ref{tab:edges}, the weight of the edge with distance $2$ from the node \emph{with$|$P}  to the node  \emph{smile$|$N} would increase by $(0.9963+0.9712)/2$, since the confidence score of the POS tag for the token \emph{with} is  $0.9963$ and the confidence score of the POS tag of the token \emph{smile} is $0.9712$ as shown in Table~\ref{tab:postags}.
%using the given POS tags and accuracies from Table~\ref{tab:postags}, the increase in the weights would be respectively $0.9963+0.9712/2$, $0.998+0.9712/2$ and %$0.9971+0.9712/2$.

\begin{table}[hbt]
  \caption{Example edges extracted from the sample phrase  ``with a beautiful smile''.}
  \centering
  \begin{tabular}[tc]{|l|l|l|l|}
    \hline
\textbf{from} & \textbf{to} & \textbf{distance} & \textbf{weight} \\ \hline
 with$|$P &  smile$|$N & 2 & 89 \\ \hline
 a$|$D & smile$|$N & 1 & 274 \\ \hline
 beautiful$|$A & smile$|$N & 0 & 305 \\ \hline
\end{tabular}
\label{tab:edges}
\end{table}

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.5]{fig/edges}
\caption{Sample nodes and edges from the word association graph.}
\label{fig:edges}
\end{center}
\end{figure}


\section{Graph Based Contextual Similarity}

Our graph based contextual similarity method is based on the assumption that an IV word that is the canonical form of an OOV word appears in the same context with the corresponding OOV word. In other words, the two nodes in the graph share several neighbors that co-occur within the same distances to the corresponding two words in social media text. We also assume that an OOV word and its canonical form should have the same POS tag.

Given an input text for normalization, the next step after preprocessing is finding the normalization candidates for each OOV token in the input text. For each ill-formed OOV token $o_i$ in the input text, first the list of tokens that co-occur with $o_i$ in the input text and their positional distances to $o_i$ are extracted.  This list is called the neighbor list of token $o_i$, i.e.,  $\text{NL}(o_i)$. Table~\ref{tab:neigh} shows a sample neighbor list for the OOV token beatiful$|$A from the sample sentence in Table~\ref{tab:postagged}.

\begin{table}[hbt]
\caption{Example neighbor list for the OOV node beatiful$|$A.}
\centering
\begin{tabular}{|l|l|l|}
    \hline
    \textbf{id} & \textbf{tag} & \textbf{position} \\
    \hline
    w & P & -2 \\     \hline
    a & D & -1 \\     \hline
    smile & V & 1 \\     \hline
  \end{tabular}
\label{tab:neigh}
\end{table}

For each neighbor node $n_{j}$ in $\text{NL}$, the word association graph is traversed, and the edges from or to the node $n_{j}$ are extracted. The resulting edge list $\text{EL}(o_i)$ has edges in the form of~($n_{j}$, $c_{k}$) or~($c_{k}$, $n_{j}$), where $c_{k}$ is a candidate canonical form of the OOV word $o_i$.
Here the neighbor node $n_{j}$ can be an OOV node, but the candidate node $c_{k}$ is chosen among the IV nodes.
The edges in $\text{EL}(o_i)$ are filtered by the relative distance of $n_{j}$ to $o_i$ as given in the $\text{NL}(o_i)$. Any edge between  $n_{j}$ and $c_{k}$, whose distance is not the same as the distance between $n_{j}$ and $o_i$ is removed.

In addition to distance based filtering, POS tag based filtering is also performed on the edges in $\text{EL}(o_i)$. Each candidate node should have the same POS tag with the corresponding OOV token. For the OOV token $o_i$ that has the POS tag $T_i$, all the edges that include candidates with a tag other than $T_i$ are removed from the edge list $\text{EL}(o_{i})$. Thus, $\text{EL}(o_{i})$ only contains edges where candidate nodes are tagged as $T_i$.

Figure~\ref{fig:edgeWeight} represents a portion from the graph where you can see the neighbours  and candidates of the OOV node ``beatiful''. In the sample sentence in Table~\ref{tab:postagged} there is two OOV token to be normalized, $o_1=w$ and $o_2=beatiful$. The neighbour list of $o_2$, $\text{NL}(o_2)$ includes  $n_1=with$, $n_2=a$ and $n_3=smile$. For each neighbor in the $\text{NL}(o_2)$, the candidate nodes~($c_1=broken$, $c_2=nice$, $c_3=new$, $c_4=beautiful$, $c_5=big$, $c_6=nice$, $c_7=great$) are extracted. As shown in Figure~\ref{fig:edgeWeight}, there are 11 lines representing the edges between the neighbors of the OOV token and the candidate nodes. These are representative edges in the $\text{EL}(o_2)$. Each member of the edge list has the same tag (A for Adjective) as the OOV node ``beatiful'' and each has the same distance to the neighbor node they are connected as the OOV node. We are simply looking for the best replacements using the distance and POS tag properties.

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.5]{fig/edgeWeight}
\caption{A portion of the graph that includes the OOV token ``beatiful'', its neighbors and the candidate nodes that each neighbor is connected to. Thick lines shows the edge list with relative weights.}
\label{fig:edgeWeight}
\end{center}
\end{figure}

Each edge in $\text{EL}(o_i)$ consists of a neighbor node $n_j$, a candidate node $c_k$ and an edge weight $\text{edgeWeight}(n_j,c_k)$. The edge weight represents the likelihood or the strength of association between the neighbor node $n_j$ and the candidate node $c_k$. As described in the previous section the edge weights are computed based on the frequency of co-occurrence of two tokens, as well as the confidence scores of their POS tags.

The edge weights of the edges in $\text{EL}(o_2)$ are shown in Figure~\ref{fig:edgeWeight}. The edges that are connected to the OOV neighbor ``w'' have smaller edge weights such as 3,5,26. On the other hand, the edges that are connected to common words have higher edge weight~(e.g.~the edge weight of the edge between nodes ``a'' and ``new'' is 24388). This indicates that those words are non OOV and more common words, and they co-occur very often in the same form (``a new'').

%As shown in Equation~\ref{eq:ew}, we assume that the direction of the edge between two tokens doesn't affect the strength of association between them~(i.e. the edge weight $w$).
Although this edge weight metric is reasonable for identifying the most likely canonical form for the OOV word $o_i$, it has the drawback of favoring words with high frequencies like these common words or stop words. Therefore, to avoid overrated words and get contextually relative candidates, we normalize the edge weight $\text{edgeWeight}(n_j,c_k)$ with the frequency of the candidate node $c_k$ as shown in Equation~\ref{eq:ew_norm}.
\begin{equation}
edgeWeightNormalized(n_j,c_k) = edgeWeight(n_j,c_k) / frequency(c_k)
\label{eq:ew_norm}
\end{equation}

Equation~\ref{eq:ew_norm} provides a metric that captures contextual similarity based on binary associations.
%However we need more than binary relatedness to achive a comprehensive contextual coverage.
In order to achieve a more comprehensive contextual coverage, a contextual similarity feature is built based on the sum of the binary association scores of several neighbors. As shown in Equation~\ref{eq:wscore1}, for a candidate node $c_k$ the total edge weight score is the sum of the normalized edge weight scores $\text{edgeWeightNormalized}(n_j,c_k)$, which are the edge weights coming from the different neighbors of the OOV token $o_i$. We expect this contextual similarity feature to favor and identify the candidates which are (1) related to many neighbors, and (2) have a high association score with each neighbor.
\begin{equation}
\text{edgeWeightScore}(o_i,c_k) = \sum_{(n_j,c_k) \text{or} (c_k,n_j) \in \text{EL}(o_i)}{\text{edgeWeightNormalized}(n_j,c_k)} \\\\
\label{eq:wscore1}
\end{equation}

Figure~\ref{fig:contextscores} includes the top three candidates sorted by their scores. The candidate node ``beautiful'', $c_4$, has a frequency of 17900. Both three neighbors were paired in the edge list with it with the weights 2, 2918 and 305 respectively~(Figure~\ref{fig:edgeWeight}). After normalizing the edge weight by the frequency of the candidate node  $c_4$, sum of those normalized weights gives us the edge weight score of the candidate node: $\text{edgeWeightScore}(o_2,c_4) = 0.18$.

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.5]{fig/contextScores}
\caption{Candidates for ``beatiful'' sorted by their edge weight scores.}
\label{fig:contextscores}
\end{center}
\end{figure}

Our word association graph includes both OOV and IV tokens, and our OOV detection depends on the spellchecker which fails to identify some OOV tokens that have the same spelling with an IV word. In order to propose better canonical forms, the frequencies of the normalization candidates in the social media corpus have also been incorporated to the contextual similarity feature. Nodes with higher frequencies lead to tokens that are in their most likely grammatical forms.

The final contextual similarity of the token $o_i$ and the candidate $c_k$ is the weighted sum of the total edge weight score and the frequency score of the candidate~(See Equation~\ref{eq:contscore}). The frequency score of the candidate is a real number between 0 and 1. It is proportional to the frequency of the candidate with respect to the frequencies of the other candidates in the corpus. Since the total edge weight score is our primary contextual resource, we may want to favor edge weight score. For that reason we give the frequency score a weight $\lambda \leq 1$ to be able to limit its effect to the total contextual similarity score.
\begin{equation}
\text{contextSimScore}(o_i,c_k) = \text{edgeWeightScore}(o_i,c_k) + \lambda * \text{freqScore}(c_k)
\label{eq:contscore}
\end{equation}

Hereby, we have the candidate list $\text{CL}(o_{i})$ for the OOV token $o_i$ that includes all the unique candidates in $\text{EL}(o_{i})$ and their contextual similarity scores calculated.

\section{Lexical Similarity}

Following the prior work in~\cite{Han:2011:LNS:2002472.2002520,DBLP:conf/acl/HassanM13}, our lexical similarity features are based on edit distance~\cite{levenshtein1966bcc}, double metaphone~(phonetic edit distance)~\cite{Philips:2000:DMS:349124.349132}, and a similarity function~\cite{Contractor:2010:UCN:1944566.1944588} which is based on Longest Common Subsequence Ratio (LCSR)~\cite{melamed1999bitext}.

Edit distance or in other words Levenshtein distance between two words is defined as the minimum number of single character changes such as insertion, deletion and substitution to convert one word into another. Edit distance has major application in NLP especially in the spell checking.

Double metaphone is an extended word edit distance measure, that not only considers the characters but also the English pronunciations of the words. Like the Soundex algorithm, metaphone algorithm~\cite{philips1990hanging} encodes words. Similar sounding words, in other words phoneticly similar words shares the same keys in these encodings. Metaphone is an extended and more accurate version of Soundex. Double metaphone came out 10 years later than the metaphone and is called the second generation of the metaphone algorithm.

It returns two keys instead of presenting a more valid coverage for some irregularities. % fix and give more detail

The cost function we are using is defined by Contractor \textit{et al.}, 2010~\cite{Contractor:2010:UCN:1944566.1944588}, as the ratio between LCSR of two words and their Edit Distance(ED) cost. You can see the LCSR and cost function shown in Equation~\ref{eq:lcsr} and Equation~\ref{eq:lcsr_cont}. In our case we calculate the similarity cost of the candidate word and the OOV word by dividing their LCSR value to the Edit Distance between them. From now on we will refer to the simCost in Equation~\ref{eq:lcsr_cont} as $\text{LCSR\_ED}$.
\begin{align}
\text{LCSR}(o_j,c_k) &= \frac{lenght(\text{LCS}(o_j,c_k))}{\text{max}(\text{length}(o_j),\text{length}(c_k))} \label{eq:lcsr}      \\
\text{simCost} (o_j,c_k) &= \frac{\text{LCSR}(o_j,c_k)}{\text{ED}(o_j,c_k)} \label{eq:lcsr_cont}
\end{align}
Following the tradition that is inspired from~\cite{Kaufmann2010} before lexical similarity calculations, any repetitions of characters three or more times in OOV tokens are reduced to two (e.g. \emph{goooood} is reduced to \emph{good}). Then, the edit distance, phonetic edit distance, and $\text{LCSR\_ED}$ between each candidate in $\text{CL}(o_{i})$ and the OOV token $o_i$ are calculated. Edit distance and phonetic edit distance are used to filter the candidates. Any candidate in $\text{CL}(o_{i})$ with an edit distance greater than $t_{edit}$ and phonetic edit distance greater than $t_{phonetic}$ to $o_i$ has been removed from the candidate list $\text{CL}(o_{i})$.

For the remaining candidates, the total lexical similarity score (Equation~\ref{eq:lexscore}) is calculated using $\text{LCSR\_ED}$ and edit distance score\footnote{an approximate string comparison measure (between 0.0 and 1.0) using the edit distance \url{https://sourceforge.net/projects/febrl/}}. Similar to contextual similarity score, here we have one main lexical similarty feature and one minor lexical similarity feature. The major lexical similarity feature is $\text{LCSR\_ED}$ and edit distance score is the minor. We assigned a weight $\beta \leq 1$ to the edit distance score to be able to lower its weight while calculating the total lexical similarty score.
\begin{equation}
\text{lexSimScore}(o_i,c_k) = \text{LCSR\_ED}(o_i,c_k) + \beta * \text{editDistScore}(o_i,c_k)
\label{eq:lexscore}
\end{equation}
\par Since some social media text messages are extremely short and contain several OOV words, they do not provide sufficient context, i.e., IV neighbors, to enable the extraction of good candidates from the word association graph. Therefore, we extended the candidate list obtained through contextual similarity as described in the previous section, by including all the tokens in the word association graph that satisfy the edit distance and phonetic edit distance criteria. We also incorporated candidates from external resources, in other words from a slang dictionary and a transliteration table of numbers and pronouns (Table ~\ref{tab:transliteral}). If a token occurs in the slang dictionary or in the transliteration table it is assigned an external score of $1$, otherwise it is assigned an external score of $0$.

\begin{table}[ht]
  \caption{Transliteration Candidates extended from~\cite{Gouws:2011:CBL:2021109.2021113}.}
  \begin{minipage}[c]{0.5\linewidth}
    \begin{tabular}[l]{|l|l|l|}
    \hline
    \textbf{token} & \textbf{tag} & \textbf{Transliteration} \\\hline
    1 & ``\$'' & ``one'' \\\hline
    2 & ``\$'' & ``two'' \\\hline
    3 & ``\$'' & ``three'' \\\hline
    4 & ``\$'' & ``for'' \\\hline
    5 & ``\$'' & ``five'' \\\hline
    6 & ``\$'' & ``six'' \\\hline
    7 & ``\$'' & ``seven'' \\\hline
  \end{tabular}
\end{minipage}
  \begin{minipage}[c]{0.5\linewidth}
    \begin{tabular}[l]{|l|l|l|}
    \hline
    \textbf{token} & \textbf{tag} & \textbf{Transliteration} \\\hline
    8 & ``\$'' & ``eight'' \\\hline
    9 & ``\$'' & ``nine'' \\\hline
    0 & ``\$'' & ``zero'' \\\hline
    2 & ``P''  & ``to'' \\\hline
    ``w'' & ``P''  & ``with'' \\\hline
    ``im'' & ``L''  & ``I'm'' \\\hline
    ``cont'' & ``\textasciitilde''  & ``continued'' \\\hline
  \end{tabular}
  \end{minipage}
\label{tab:transliteral}
\end{table}

%\begin{verbatim}
%units = ["", "one", "to", "three", "for",
%         "five", "six", "seven", "eight", "nine"]
%pronouns = {u'2':u"to",u'w':u"with"}
%\end{verbatim}

As shown in Equation~\ref{eq:candscore}, the final score of a candidate IV token $c_k$ for an OOV token $o_i$ is the sum of its lexical similarity score, contextual similarity score and external score with respect to $o_i$.
\begin{equation}
\begin{aligned}
\text{candScore}(o_i,c_k) = ~ & \text{lexSimScore}(o_i,c_k) + \text{contextSimScore}(o_i,c_k) \\
& + \text{externalScore}(o_i,c_k)
\end{aligned}
\label{eq:candscore}
\end{equation}

\chapter{EXPERIMENTS}
\label{sec:experiments}

\section{Data sets}
We used the LexNorm1.1 dataset~\cite{Han:2011:LNS:2002472.2002520} and Pennell \textit{et al.}'s trigram dataset~\cite{pennell2011character} to evaluate our proposed approach. LexNorm1.1 contains $549$ tweets with $1184$ manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work. Trigram dataset on the other hand is an SMS-like corpus collected from twitter status updates sent via SMS.\@ The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated, previous work has used 80 \% for training and the rest as test set, similarly we used the same 20 \% of the dataset as test set to be able to report our results on the same basis as previous work.

\section{Graph Generation}
We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford's 476 million Twitter Dataset~\cite{DBLP:conf/wsdm/YangL11}. The language identification of tweets was performed by using the langid.py Python library~\cite{Lui:2012:LOL:2390470.2390475, Baldwin:2010:LIL:1857999.1858026}.

CMU Ark Tagger, which is a social media specific POS tagger achieving an accuracy of $95\%$ over social media text~\cite{owoputi2013improved,Gimpel:2011:PTT:2002736.2002747}, is used for tokenizing and POS tagging the tweets. Besides the standard POS tags, the POS tagset of the Ark Tagger includes some extra POS tags specific to social media including URLs and emoticons; Twitter hashtags~(\#); and twitter at-mentions~(@). One other tag that is special to social media is ``\textasciitilde'' which means the token is specific to a discourse function of twitter such as \textit{rt, cont.}. Lastly G stands for miscellaneous words including multi word abbreviations like btw~(by the way), nw~(no way), and smh~(somehow).

We made use of these social media specific tags to disambiguate some OOV tokens. For example if OOV token ``cont'' is tagged with the discourse function tag G, we added ``continued'' to the candidate list as an external node.

After tokenization, we removed the tokens that were POS tagged as mention~(e.g. @brendon), discourse marker (e.g. RT), URL, email address, emoticon, numeral and punctuation. The remaining tokens are used to build the word association graph. After constructing the graph we only kept the nodes with a frequency greater than $8$. For the performance related reasons, the relatedness thresholds $t_{distance}$ and $t_{frequency}$  were chosen as $3$ and $8$, respectively. The resulting graph contains $105428$ nodes and $46609603$ edges.

\section{Candidate Set Generation}

While extending the candidate set with lexical features we use ${t_{edit}\leq 2}~\vee~{t_{phonetic} \leq 1}$ to keep up with the settings in Han \textit{et al.}~\cite{Han:2011:LNS:2002472.2002520}. In other words, IV words that are within 2 character edit distance of a given OOV word or 1 character edit distance of a given OOV word under phonemic transcription were chosen as lexical similarity candidates.

\section{Evaluation Metrics}

\chapter{RESULTS AND ANALYSIS}

The results obtained by our proposed Contextual Word Association Graph (CWA-Graph) system on the LexNorm1.1 dataset,
as well as the results of recent studies that used the same data set for evaluation are presented in Table~\ref{tab:results}.

\begin{table}[thb]
  \caption{Results obtained on the LexNorm1.1 dataset.}
  \centering
  \begin{tabular}[t]{|l|c|c|c|}
    \hline
    \textbf{Method} & \textbf{Precision} & \textbf{Recall} & \textbf{F-measure} \\
    \hline
    Han and Baldwin, 2011 & 75.30 & 75.30 & 75.30 \\\hline
    Liu \textit{et al.}, 2011 & 84.13 & 78.38 & 81.15 \\\hline
    Hassan \textit{et al.}, 2013 & 85.37 & 56.40 & 69.93 \\\hline
    Yang \textit{et al.}, 2013 & 82.09 & 82.09 & 82.09 \\\hline
    CWA-Graph   & 85.50 & 79.20 & 82.20 \\
    \hline
  \end{tabular}
  \label{tab:results}
\end{table}

Our CWA-Graph approach achieves the best f measure ($82.20$) and precision ($85.50$) among the recent previous studies. The high precision value is obtained without compromising much from recall ($79.20$), our precision is the second best among others. The F-score ($82.09$) obtained by Yang \textit{et al.}'s system is close to ours and the second best F-score, which on the other hand, has a lower precision than our approach~\cite{DBLP:conf/emnlp/YangE13}.

The earlier work we compare our system with, assumes that the words to be normalized are given in advance. We also made the same assumption. However unlike other systems (\cite{DBLP:conf/emnlp/YangE13,liu2012broad,Han:2011:LNS:2002472.2002520}), our system does not propose a normalization, if there are no candidates that are lexically similar, grammatically correct and contextually close enough. For this reason, we managed to achieve a higher precision compared to the other systems. Besides, we made sure that the candidates have a minimum similarity either contextual, lexical, external or some degree of each feature. Table~\ref{tab:thresholds} shows that our approach can obtain even higher values of precision by tuning the system threshold (i.e.~the minimum score in Equation~\ref{eq:candscore} to return a token as a candidate canonical form of an OVV token).

\begin{table}[thb]
  \caption{Comparison of results for different threshold values.}
  \centering
  \begin{tabular}[th]{|l|c|c|c|}
    \hline
    \textbf{Threshold} & \textbf{Precision} & \textbf{Recall} & \textbf{F-measure} \\
    \hline
    $\leq$ 1 & 81.2 &	80.8 &	81 \\\hline
    1.1 & 81.5 & 80.8 &	81.2 \\\hline
    1.2 & 82.2 &	80.7 &	81.4 \\\hline
    1.3 & 83.7 &	80.2 &	81.9 \\\hline
    1.4 & 84.2 &	80.0 &	82.0 \\\hline
    1.5 & \textbf{85.5} &  \textbf{79.2} &  \textbf{82.2} \\\hline
    1.6 & 88.8	& 75.1	& 81.4 \\\hline
    1.7	& 91.1	& 72.8	& 80.9 \\\hline
    1.8	& 92.3	& 67.6	& 78 \\\hline
    2	& 94.1	& 56.4	& 70.5 \\
    \hline
  \end{tabular}
\label{tab:thresholds}
\end{table}

We also test our system on different window sizes. The window size is defined by the number of total neighbours of an OOV word in the given text. When we run our system with a contextual association threshold $t_{distance}=3$, which means two words are considered as contextually associated, if they are within a maximum word distance of 3 in the text, 3 words to the left and 3 words to the right are taken into account for finding contextually similar candidates. For example when we look at the first example in Table~\ref{tab:windowsize}, the $t_{distance}$ is set to 3 and  for the OOV word ``w'', the window size is 7. On the other hand, in the second example, OOV word ``beatiful'' has 3 neighbors on the left but only one neighbour on the right, ending up with a window size 5 when the maximum window size set to 7. Thus the window size is defined by our threshold is only a maximum value, since the OOV word may or may not have enough neighbors to fit the maximum window size. From now on we will refer to maximum window size as just window size.

\begin{table}[thb]
  \caption{Window size examples from our sample sentence from Table~\ref{tab:postagged} for OOV words ``w'' and ``beatiful'' with $t_{distance}=3$ and $t_{distance}=2$.}
  \centering
  \begin{tabular}{|c|*9{c}|}
    \hline
    \textbf{Max window size} & \multicolumn{9}{c|}{\textbf{Sentence}} \\
    \hline & & & & & & & & &  \\ \cline{3-9}

    7 & Let's & \multicolumn{1}{|c}{\underline{start}} & \underline{this} & \underline{morning} & \textbf{w} & \underline{a} & \underline{beatiful} & \multicolumn{1}{c|}{\underline{smile}} & . \\
    \cline{3-9} & \multicolumn{9}{c|}{} \\ [0.01cm]
    \hline  & \multicolumn{9}{c|}{} \\ \cline{5-9}

    7 & Let's & start & this & \multicolumn{1}{|c}{\underline{morning}} &\underline{w} & \underline{a} & \textbf{beatiful} & \multicolumn{1}{c|}{\underline{smile}} & . \\
    \cline{5-9} & \multicolumn{9}{c|}{} \\ [0.01cm]
    \hline  & & & & & & & & & \\ \cline{4-8}

    5 & Let's & start & \multicolumn{1}{|c}{\underline{this}} & \underline{morning} & \textbf{w} & \underline{a} & \multicolumn{1}{c|}{\underline{beatiful}} & smile & . \\
    \cline{4-8} & & & & & & & & & \\ [0.01cm]
    \hline  & & & & & & & & & \\ \cline{6-9}

    5 & Let's & start & this & morning & \multicolumn{1}{|c}{\underline{w}} & \underline{a} & \textbf{beatiful} & \multicolumn{1}{c|}{\underline{smile}} & . \\
    \cline{6-9} & & & & & & & & & \\ [0.01cm]
    \hline
  \end{tabular}
\label{tab:windowsize}
\end{table}

Considering twitter's limit on message lenght and users' tendency of using url and long hastags but short texts, the optimal window size is as expected relatively small. As shown in Table~\ref{tab:windows}, the system achives best results with a window size of 7. This is also the setup we have used for our experiments. With higher window sizes OOV tokens get context information from outer word phrases which include contextually less relevant words. Choosing a smaller window size shortens the execution time, but on the other hand this results in missing some important contextual information.

\begin{table}[thb]
  \caption{Comparison of results for different window sizes.}
  \centering
  \begin{tabular}[th]{|l|c|c|c|}
    \hline
    \textbf{Window size} & \textbf{Precision} & \textbf{Recall} & \textbf{F-measure} \\
    \hline
    3 & 85.3 & 79.0 & 82.0 \\\hline
    5 & 85.6 & 79.1 & 82.2 \\\hline
    7 & \textbf{85.5} &  \textbf{79.2} &  \textbf{82.2} \\\hline
    9 & 85.2 & 79.0  & 82.0 \\\hline
  \end{tabular}
\label{tab:windows}
\end{table}

Table~\ref{tab:resultspennell} show the results of our system on the trigram SMS-like dataset. Without any modification to our system or to the parameters, we were able to improve results of Pennell \textit{et al.}~\cite{pennell2011character}.

\begin{table}[thb]
  \caption{Results obtained on the trigram SMS-like dataset.}
  \centering
  \begin{tabular}[t]{|l|c|c|c|}
    \hline
    \textbf{Method} & \textbf{Precision} & \textbf{Recall} & \textbf{F-measure} \\
    \hline
    Pennell \textit{et al.}, 2011 & 69.70 & 69.70 & 69.70 \\\hline
    CWA-Graph   & 78.20 & 68.50 & 73.10 \\\hline
  \end{tabular}
  \label{tab:resultspennell}
\end{table}

\chapter{CONCLUSION AND FUTURE WORK}

We presented an unsupervised graph based approach for contextual text normalization. We compared our approach with the recent social media text normalization systems and achieved state-of-the-art precision and F-measure scores.

The proposed approach can analyze grammatical and contextual information from the noisy input text. The task of normalization is highly dependent on understanding and capturing the dynamics of informal nature of noisy text. Our word association graph is built using a large unlabeled social media corpus. It helps to derive contextual and grammatical analysis on both clean and noisy data.

Except for the double metaphone algorithm that encodes the phonetic similarities among words in English, the proposed approach is highly language independent. As future work, we will apply our system to different languages.
% \nocite{NewEntry1,NewEntry2,NewEntry3,NewEntry4,NewEntry5,
% NewEntry6,
% NewEntry7,NewEntry8,NewEntry9,NewEntry10,NewEntry11,NewEntry12}
%\appendix
%\chapter{APPLICATION}
%The appendices start here.
%\cite{*}
\bibliographystyle{styles/fbe_tez_v11}
\bibliography{sentic}

\end{document}


\begin{table}[tbhp]
\caption{Sample tokenized, POS tagged sentence and the corresponding nodes and edges in the word association graph.(L:nominal+verbal, V:verb, D:determiner, N:noun, A:adjective, C:punctuation).}
\begin{minipage}[c]{\linewidth}
\fxbox{
Let's$_{\textcolor{red}{L}}$ start$_{\textcolor{red}V}$ this$_{\textcolor{red}D}$ morning$_{\textcolor{red}N}$ w$_{\textcolor{red}P}$ a$_{\textcolor{red}D}$ beatiful$_{\textcolor{red}A}$ smile$_{\textcolor{red}N}$.$_{\textcolor{red}C}$
 }\par
\vspace{5mm}
\end{minipage}
\begin{minipage}[c]{\linewidth}
\begin{tabular}[h]{l|l}
Tokens & Let's, start, this, morning, w, a, beatiful, smile,~. \\
\hline
Nodes & Let's$|$L, start$|$V, this$|$D, morning$|$N, w$|$P, a$|$D, beatiful$|$A, smile$|$N, ,$|$C \\
\hline
Edges & \{Let's$|$L, start$|$V , distance:0\},\{Let's$|$L, this$|$D, distance:1\}, \\
& ... \\
& \{a$|$D, beatiful$|$A, distance:0\}, \{a$|$D, smile$|$N, distance:1\}, \\
& \{beatiful$|$A, smile$|$N, distance:0\} \\
\end{tabular}
\end{minipage}
\label{tab:graph}
\end{table}
