\documentclass[a4paper,onesided,12pt]{report}
\usepackage{styles/fbe_tez}
\usepackage[utf8x]{inputenc} % To use Unicode (e.g. Turkish) characters
\renewcommand{\labelenumi}{(\roman{enumi})}
\usepackage{amsmath, amsthm, amssymb}
 % Some extra symbols
\usepackage[bottom]{footmisc}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{longtable}
\graphicspath{{figures/}} % Graphics will be here

\usepackage{multirow}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
%\pagestyle{empty}
%\includeonly{introduction} % To only process the given file

\usepackage{color,url,amsmath,tabularx}
\newcommand\fxbox[1]{\center{\framebox[1.1\width][c]{\strut#1}}}
\newcommand\mxbox[1]{\center{\makebox[1.1\width][c]{\strut#1}}}


\newtheorem{thm}{Theorem}[chapter]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
% COVER PAGE
\title{A Graph Based Approach for Contextual Text Normalization}
\turkcebaslik{Metin Normalizasyonu}
\degree{M.S, Computer Engineering, Boğaziçi University, 2014}
\author{Çağıl Uluşahin Sönmez}
\program{Computer Engineering}
\subyear{2013}

% APPROVED BY PAGE
\supervisor{Assist. Prof. Arzucan Özgür }
%\cosuperi{Title and Name of Cosupervisor I}
%\cosuperii{Title and Name of Cosupervisor II}
\examineri{Prof.~Tunga Güngör}
\examinerii{Assist. Prof.~Gülşen Cebiroğlu Eryiğit}
%\examineriv{}
%\examinerv{}
\dateofapproval{10.04.2014}

\begin{document}

\pagenumbering{roman}
\makemstitle % M.S. thesis
\makeapprovalpage
\begin{acknowledgements}
Acknowledgements come here...
\end{acknowledgements}
\begin{abstract}
The informal nature of social media text render is very difficult to be automatically processed by natural language processing tools. Text normalization, which corresponds to restoring the noisy words to their canonical forms, provides a solution to this challenge.
We introduce an unsupervised text normalization approach that utilizes not only lexical, but also contextual and grammatical features of social text.
The contextual and grammatical features are extracted from a word association graph built by using a large unlabeled social media text corpus.
The graph encodes the relative positions of the words with respect to each other, as well as their part-of-speech tags.
The lexical features are obtained by using the longest common subsequence ratio and edit distance measures to encode the surface similarity among words, and the double metaphone algorithm to represent the phonetic similarity. Unlike most of the recent approaches that are based on generating normalization dictionaries, the proposed approach performs normalization by considering the context of the noisy words in the input text.
%In other words, a noisy word can be normalized to different canonical forms depending on the context of the word in the input text message.
Our results show that it achieves state-of-the-art F-score performance on a standard data set. In addition, the system can be tuned to achieve very high precision without sacrificing much from recall.
\end{abstract}
\begin{ozet}
Bir sayfa uzunluğunda özet gelecektir.
\end{ozet}
\tableofcontents
\listoffigures
\listoftables
\begin{symbols}
% The title will be typeset as "LIST OF SYMBOLS".
%
% Use a separate \sym command for each symbols definition.
% First Latin symbols in alphabetical order

\sym{$a_{ij}$}{Description of $a_{ij}$}
\sym{$\mathbf{A}$}{State transition matrix of a hidden Markov model}
% Then Greek symbols in alphabetical order
\sym{}{}
\sym{$\alpha$}{Blending parameter \textit{or} scale}
\sym{$\beta_t(i)$}{Backward variable}
\sym{$\Theta$}{Parameter set}
\sym{ }{}

\end{symbols}

\begin{abbreviations}
 % Abbreviations in alphabetical order
\sym{2D}{Two Dimensional}
\sym{3D}{Three Dimensional}
\sym{AAM}{Active Appearance Model}
\sym{ASM}{Active Shape Model}
\end{abbreviations}


\chapter{INTRODUCTION}
\label{chapter:introduction}
\pagenumbering{arabic}
Within the last decade, the common belief among internet users, that social text has (or should have) it's own lexical and grammatical features, has naturally given birth to an internet language and jargon; which has been steadily growing and evolving ever since ~\cite{Choudhury:2007:IMS:1326044.1326048, eisenstein2013bad}. This behavioral preference phenomenon brings another challenge of its own. Not only is the internet jargon itself growing and evolving in an exponential pace, but also since the beginning of World Wide Web, internet has it's own slang. \textit{lol} meaning \textit{laughing out loudly},  \textit{xoxo} meaning \textit{kissing}, \textit{4u} meaning \textit{for you} are the among the most commonly used examples of this slang. In addition, these specific forms of informal expressions in social text usually take many different lexical forms when generated by each individual; even though the intended contextual meaning might be the same~\cite{eisenstein2013bad}. In other words, with each different individual the same content is being expressed (written) in different ways. Due to this unpredicted variety of such expressions, it would be appropriate to call this divergency "noise" in social text.

The scope of the problem doesn't end there. In addition, within the last few years, by the increasing use of mobile devices, social text has now been preferred to be transcribed by using Speech-to-Text (STT) tools. This text input preference is getting trendier and being used more frequently. The insufficient accuracy of such STT tools brings considerable amount of ``additional noise'' to social text. Tools such as spellcheckers and slang dictionaries have proven to be insufficient to cope with this challenge long time ago~\cite{sproat2001normalization}.

Lastly, when we also consider the usual scarcity of attention when people post messages on social media platforms, the problem of analyzing social text actually goes beyond the reach of human cognitive capacity. The mass usage of such social media platforms, makes it impossible to derive analysis results in a limited time scope when processed manually. In addition, most automatic Natural Language Processing (NLP) tools such as named entity recognizers and  dependency parsers generally perform poorly on social media text~\cite{ritter2010unsupervised}.

\begin{table}[hbp]
\caption[Sample tweets and their normalized forms.]{Sample tweets and their normalized forms.}
\label{tab:sentences}
\begin{tabular}{|>{\itshape}p{7cm}|p{7cm}|}
\hline
Its a beautiful nite, lukin for smth fun to do, I think I wanna be w ma frnds. &
It’s a beautiful night, looking for something fun to do, I think I want to be with my friends. \\
\hline
Dnt always follow da crowd, stand 4 wat u blv in &
Don't always follow the crowd, stand for what you beleive in. \\
\hline
There r sm songs u don't want 2 listen 2 yl walking cos when u start dancing ppl won't knw y. &
There are some songs you don't want to listen to while walking because when you start dancing people won't know why. \\
\hline
@Cloudy me tht go be sad wen the hangover hold me tmr! &

@Cloudy me that going to be sad when the hangover hold me tomorrow! \\
\hline
I srsly need some legend of korra raight nao \#linplz &
I seriously need some legend of korra right now \#linplz \\
\hline
My landlady is askin me 4 money 2.. I dnt knw wat 2 do cos I didn't stay in d house.. &
My landlady is asking me for money too.. I don’t know what to do cause I didn't stay in the house.. \\ \hline
Wat was tht for u lil shit, dnt u draw on my enlgand &
What was that for you little shit, don’t you draw on my England \\ \hline
Hav guts to say wat u desire.. Dnt beat behind da bush!! And 1 mre thng no mre say u r people's man!! &
Have guts to say what you desire.. Don’t beat behind the bush!! And one more thing no more say you are people's man!! \\
\hline
\end{tabular}
\end{table}

Text normalization is a preprocessing step to restore noisy words in text to their original (canonical) forms~\cite{Han:2011:LNS:2002472.2002520} to make use in NLP applications or more broadly to understand the digitized text better. For example, \textit{talk 2 u later} can be normalized as \textit{talk to you later} or similarly \textit{enormoooos, enrmss, enourmos} can be normalized as \textit{enormous}. You can find more examples of normalized text in Table~\ref{tab:sentences}. These noisy tokens are referred as Out of Vocabulary (OOV) words. The normalization task restores the OOV words to their In Vocabulary~(IV) forms. Table \ref{tab:normalizations} shows sample OOV words encountered in social media text and their corresponding IV forms.

\begin{table}[tbhp]
\caption{Sample noisy tokens and their normalized forms.}
\centering
\begin{minipage}[c]{.4\linewidth}
\begin{tabular}[h]{|p{2cm}|p{2cm}|}
\hline
ppl & people \\
havent & haven't \\
tmr &   tomorrow \\
soooo &  so \\
sooon &  soon \\
raight & right \\
raight & alright \\
\hline
\end{tabular}
\end{minipage}
\begin{minipage}[c]{.4\linewidth}
\centering
\begin{tabular}[h]{|p{2cm}|p{2cm}|}
\hline
r  &  are \\
mor &    more \\
doin &   doing \\
n &      and \\
friiied &  fried \\
finge &  finger \\
makeing & making \\
\hline
\end{tabular}
\end{minipage}
\label{tab:normalizations}
\end{table}

Every OOV word should not be considered for normalization. The social text is continuously evolving with new words and named entities that are not in the vocabularies of the systems~\cite{DBLP:conf/acl/HassanM13}. The OOV tokens that should be considered for normalization are referred to as ill-formed words. Oppositely an OOV word can sometimes lexically fit an IV word (Ex:~\textit{tanks} is both an IV word and an OOV word with the canonical form \textit{thanks}).
%The task of recognizing which tokens are OOV, and which of those are ill-formed are beyond the scope of this paper.

In~\cite{Choudhury:2007:IMS:1326044.1326048} Choudhury et Al.~proposes that the OOV words observed in noisy text can be classified into two groups, unintentional and intentional errors. The unintentional errors are caused by (1) pressing of the wrong key, (2) pressing of a key more than the desired number of times, (3) deletion of a character or (4) inadequate knowledge of spelling. As for the intentional errors, they can be categorized into four categories: character deletion(``tlk'' for ``talk'', ``msg'' for ``message'', ``tomoro'' for ``tomorrow'', ``mob'' for ``mobile''), phonetic substitution~(``nite'' for ``night'', ``bk'' for ``back'', ``u'' for ``you'', ``m8'' for ``mate''), abbreviations~(``btw'' for ``by the way'', ``kgp'' for ``Kharagpur'') and non-standard usage~(``wanna'' for ``want to'', ``betta'' for ``better'', ``sumfin'' for ``something'', ``b/c'' for ``because'').

In this paper, we propose a graph based text normalization method that utilizes both contextual and grammatical features of social text. The contextual information of words is modeled by a word association graph that is created from a large social media text corpus. The graph represents the relative positions of the words in the social media text messages and their Part-of-Speech (POS) tags. The lexical similarity features among the words are modeled using the longest common subsequence ratio and edit distance that encode the surface similarity and the double metaphone algorithm that encodes the phonetic similarity. The proposed approach is unsupervised, which is an important advantage over supervised systems, given the continuously evolving language in the social media domain. The same OOV word may have different appropriate normalizations depending on the context of the input text message. Recently proposed dictionary-based text normalization systems perform dictionary look-up and always normalize the same OOV word to the same IV word regardless of the context of the input text ~\cite{Han:2011:LNS:2002472.2002520,DBLP:conf/acl/HassanM13}. On the other hand, the proposed approach does not only make use of the general context information in a large corpus of social media text, but it also makes use of the context of the OOV word in the input text message. Thus, an OOV word can be normalized to different IV words depending on the context of the input text. Another strength of the proposed system is that it achieves the state-of-the art precision scores, without sacrificing from recall.

\chapter{RELATED WORK}
\label{chapter:related}

Early work on text normalization mostly made use of the noisy channel model. The first work that had a significant performance improvement over the previous research was by Brill and Moore, 2000~\cite{Brill:2000:IEM:1075218.1075255}. They proposed a novel noisy channel model for spell checking based on string to string edits. Their error model depended on probabilistic modeling of sub-string transformations. Toutanova et al., 2002 improved this approach by extending the error model with phonetic similarities over words~\cite{Toutanova:2002:PMI:1073083.1073109}.

Choudhury et al., 2007 developed a supervised Hidden Markov Model based approach for normalizing SMS Texts~\cite{Choudhury:2007:IMS:1326044.1326048}. Cook and Stevenson, 2009 have extended this model by introducing an unsupervised noisy channel model~\cite{Cook:2009:UMT:1642011.1642021}. Rather than using one generic model for all word formations as in Choudhury et al., 2007, they used a mixture model in which each different word formation type was modeled explicitly. Aw et al., 2006 proposed a phrase-based statistical machine translation (MT) model for the normalization task~\cite{Aw:2006:PSM:1273073.1273078}. They defined the problem as translating the SMS language to the English language. Pennell and Liu, 2011~\cite{pennell2011character} on the other hand, proposed a character level MT system, that is robust to new abbreviations.

The down side of these methods were: (1) they did not consider contextual features and (2) each of them assumed that tokens have unique normalizations. However, that is not the case for the normalization task. The OOV tokens are ambiguous and without contextual information it is not possible to build models that can disambiguate transformations correctly.

Another drawback of supervised models is that they require annotated data, which is not readily available and difficult to create for social media text normalization~\cite{DBLP:conf/emnlp/YangE13}.

More recent approaches handled the text normalization task by building normalization lexicons. Han et al., 2011 developed a two phased model, where they only consider the ill-formed OOV words for normalization~\cite{Han:2011:LNS:2002472.2002520}. First a confusion set is generated using the lexical and phonetic distance features. Later, the candidates in the confusion set are ranked using a mixture of dictionary look up, word similarity based on lexical edit distance, phonemic edit distance, prefix sub-string, suffix sub-string and longest common subsequence(LCS), as well as context support metrics.

Gouws et al., 2011 on the other hand, proposed an approach that depended highly on contextual information such as the geological location of the users and the twitter client that the tweet is received from~\cite{Gouws:2011:CBL:2021109.2021113}. Using contextual metrics they modeled the transformation distributions.

Liu et al., 2012 proposed a broad coverage normalization system, which integrates an extended noisy channel model, that is based on enhanced letter transformations, visual priming, string and phonetic similarity~\cite{liu2012broad}. They try to improve the performance of the top $n$ normalization candidates by integrating human perspective modeling.
Yang and Eisenstein, 2013 introduced an unsupervised log linear model for text normalization ~\cite{DBLP:conf/emnlp/YangE13}. Their joint statistical approach uses local context based on language modeling and surface similarity. Along with dictionary based models, Yang and Eisenstein's model have obtained a significant improvement on the performance of text normalization systems.

Hassan and Menezes, 2013 generated a normalization equivalence lexicon using Markov random walks on a contextual similarity lattice~\cite{DBLP:conf/acl/HassanM13}. Our approach is different from theirs in several ways. First, our system makes use of the context of the OOV word in the input text, whereas their system is a dictionary-based method that always produces the same normalization to a given OOV word, regardless of its context in the input text. Besides the tokens themselves, we make use of the POS tags in creating the graph as well as the relative positions of the words in the social media text. Hassan and Menezes, 2013 create a bipartite graph, that is relatively more conservative in modeling the context of words. Context of a word is modeled as a window of words of size five. That is, two words to the right of a word and two words to the left of a word constitute the context of a word together. Even if one word is not the same, the context is considered to be different. On the other hand, in our graph, each neighboring token contributes to the context information of a word, which leads to both a higher recall and a higher precision.
%We select the most likely IV candidates for an OOV word by considering the strength of associations between the neighbors of the OOV word in the input text, as well as the tokens %in the graph that are strongly associated with these neighbors in the similar contextual and grammatical context.
%One of the biggest difference between our system and theirs is that they make use of a huge clean vocabulary.

%We believe several models such as the morphophonemic similarity, MT, Maximum Likelihood, etc. has their own limits. Higher performance text normalization systems should make %use of contextual analysis.


\chapter{METHODOLOGY}
\label{chapter:method}

%Unsupervised and graph based context aware model:

%In our model we made use of both lexical, contextual and shallow properties of the noisy text which makes use of a weighted token co-occurrence graph.

In this paper, we propose a graph based approach that models both contextual and lexical similarity features among an OOV word that requires normalization and candidate IV words. A high level overview of our system is shown in Figure~\ref{fig:overview}. An input text is first preprocessed by tokenizing and Part-Of-Speech (POS) tagging. If the text contains an OOV word, the normalization candidates are chosen by making use of the contextual features which are extracted from a pre-generated directed word association graph, as well as lexical similarity features. Lexical similarity features are based on edit distance, longest common subsequence ratio, and double metaphone distance. In addition, a slang dictionary is used as an external resource to enrich the normalization candidate set. The details of the approach are explained in the following sub-sections.

%%% FIGURE
\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.6]{fig/overview}
\caption{High level overview of our system}
\label{fig:overview}
\end{center}
\end{figure}

\section{Preprocessing}

%Tokenization is the first step in our system. Tokenization is the process of breaking the text into words, numbers, symbols, emoticons After tokenization, next in the pipeline is POS tagging each token using a social media POS tagger. Unlike the normal POS taggers, social media POS taggers~\cite{DBLP:conf/naacl/OwoputiODGSS13}\cite{Gimpel:2011:PTT:2002736.2002747} provide a broader set of tags that is special to the social text. By this extended set of tags we can identify tokens such as discourse markers~(rt for retweets, cont. for a tweet whose content follows up in the coming tweet) or URLs. So that we can process those tokens within their context.

Tokenization is the first step in our system. It is the process of breaking the text into tokens, which are the smallest meaningful elements such as numbers, symbols, and emoticons. After tokenization, the next step in the pipeline is Part-of-Speech (POS) tagging each token using a POS tagger specifically designed for social media text. Unlike the regular POS taggers designed for well-written newswire-like text, social media POS taggers provide a broader set of tags specific to the peculiarities of social text ~\cite{owoputi2013improved,Gimpel:2011:PTT:2002736.2002747}. Using this extended set of tags we can identify tokens such as discourse markers~(e.g. rt for retweets, cont. for a tweet whose content follows up in the coming tweet) or URLs. This enables us to better model the context of the words in social media text. You can see a sample sentence from corpus preprocessed in Table~\ref{tab:postagged}.

\begin{table}[tbhp]
\caption{Sample tokenized, POS tagged sentence (L: nominal+verbal, V: verb, D: determiner, N: noun, A: adjective, C: punctuation)}
\centering
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
Let's$_{\textcolor{red}{L}}$ & start$_{\textcolor{red}V}$ & this$_{\textcolor{red}D}$ & morning$_{\textcolor{red}N}$ & w$_{\textcolor{red}P}$ & a$_{\textcolor{red}D}$ & beatiful$_{\textcolor{red}A}$ & smile$_{\textcolor{red}N}$ & .$_{\textcolor{red}C}$\\
\hline
\end{tabular}
\label{tab:postagged}
\end{table}


As shown in Table~\ref{tab:postags}, after preprocessing, each token is assigned a POS tag with a confidence measure between 0 and 1. Later, we use these confidence scores in calculating the edge weights in our context graph. Note that even though the words \emph{w} and\emph{ beatiful} are misspelled, they are tagged correctly by the tagger, with lower confidence scores though.

\begin{table}[htbp]
\caption{Sample POS tagger output obtained by using CMU Ark Tagger~\cite{owoputi2013improved,Gimpel:2011:PTT:2002736.2002747}}
\begin{minipage}{.5\linewidth}
\begin{tabular}[h]{|l>{\itshape}lr|}
 \hline
Token & POS tag & Tag confidence \\
 \hline
with & Preposition & 0.9963 \\
 \hline
a & Determiner & 0.998 \\
 \hline
beautiful & Adjective & 0.9971 \\
 \hline
smile & Noun & 0.9712 \\
 \hline
\end{tabular}
\end{minipage}
\begin{minipage}{.5\linewidth}
\begin{tabular}[h]{|l>{\itshape}lr|}
 \hline
Token & POS tag & Tag confidence \\
 \hline
w & Preposition & 0.7486 \\
 \hline
a & Determiner & 0.9920 \\
 \hline
beatiful & Adjective & 0.9733 \\
 \hline
smile & Noun & 0.9806 \\
 \hline
\end{tabular}
\end{minipage}
\label{tab:postags}
\end{table}

\section{Graph construction}

Contextual information of words is modeled through a word association graph created by using a large corpus of social media text. The graph encodes the relative positions of the POS tagged words in the text with respect to each other. After preprocessing, each text message in the corpus is traversed in order to extract the nodes and the edges of the graph.
%The graph~(See Figure~\ref{fig:graph}) is build using a big dataset of social media text. After preprocessing, we traverse each entry in the dataset and extract nodes and edges.
A node is defined with four properties: \textit{id, oov, freq, tag}. The token itself is the \textit{id} field. The \textit{freq} property indicates the node's frequency count in the dataset. The \textit{oov} field is set to True if the token is an OOV word. Following the prior work by Han and Baldwin, 2011 we used the GNU Aspell dictionary (v0.60.6) to determine whether a word is OOV or not~\cite{Han:2011:LNS:2002472.2002520}. A portion of the graph that covers a phase from the sample sentence is shown in Figure~\ref{fig:graph}.

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.6]{fig/graph}
\caption{Portion of the word association graph for the sample sentence. (d:distance, w: edge weight)}
\label{fig:graph}
\end{center}
\end{figure}

%We define a node with four properties \textit{id, oov, freq, tag}. The token itself plus it's POS tag forms the \textit{id} field. \textit{freq} property indicates the node's frequency count in the dataset. \textit{oov} field is set to True if the token is a OOV word. Following ~\cite{Han:2011:LNS:2002472.2002520} we used GNU Aspell dictionary (v0.60.6) to determine whether a word is OOV or not.

%In the word-relatedness graph, each node is a unique set of a token and a POS tag (see Table~\ref{tab:graph}). This helps us to identify the tokens not only lexically and contextually but also (in terms of POS tags) grammatically.

In the created word association graph, each node is a unique set of a token and its POS tag. This helps us to identify the candidate IV words for a given OOV word by considering not only lexical and contextual similarity, but also grammatical similarity in terms of POS tags. For example if the token \textit{smile} has been frequently seen as a Noun or a Verb, and not in other forms in the dataset~(e.g.~Table~\ref{tab:nodes}), this provides evidence that it is not a good IV candidate as a normalization for an OOV token that has  been tagged as a Pronoun. On the other hand, \textit{smile} can be a good candidate for a Noun or a Verb OOV token, if it is lexically and contextually similar to it.

\begin{table}[hbt]
  \caption{The different nodes in the word association graph representing the token \textit{smile} tagged with different POS tags.}
  \centering
  \begin{tabular}[tc]{l}
    node id : smile , freq : 3, oov : False, tag : A \\
    node id : smile , freq : 3403, oov : False, tag : N \\
    node id : smile , freq : 2796, oov : False, tag : V \\
  \end{tabular}
\label{tab:nodes}
\end{table}

An edge is created between two nodes in the graph, if the corresponding word pair (i.e.~token/POS pair) are contextually associated. Two words are considered as contextually associated if they satisfy the following criteria:

\begin{itemize}
\item The two words co-occur within a maximum word distance of $d_t$ in a text message in the corpus.
\item Each word has a minimum frequency of $f_t$ in the corpus.
\end{itemize}

\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.7]{fig/directionality}
\end{center}
\caption{The directionality of the edges is based on the sequence of words in the text messages in the corpus.}
\vskip\baselineskip % Leave a vertical skip below the figure
\label{fig:direction}
\end{figure}

The directionality of the edges is based  on the sequence of words in the text messages in the corpus~(see Table~\ref{fig:direction}). In other words, an edge between two nodes is directed from the earlier seen token towards the later seen token. For example, Table~\ref{tab:edges} and Figure~\ref{fig:edges} shows the edges that would be derived from a text including the phrase ``with a beautiful smile''. The \textit{from} property indicates the first word and \textit{to} is the latter in the phrase. The direction and the distance together represent a unique triplet. For each pair of nodes with a specific distance there is an edge with a positive weight, if the two nodes are related. Each co-occurrence of two related nodes increases the weight of the edge between them with an average of the nodes' POS tag confidence scores in the text message considered. If we are to expand the graph with the example phrase shown in Table~\ref{tab:edges}, the weight of the edge with distance $3$ from the node \emph{with$|$P}  to the node  \emph{smile$|$N} would increase by $(0.9963+0.9712)/2$, since the confidence score of the POS tag for the token \emph{with} is  $0.9963$ and the confidence score of the POS tag of the token \emph{smile} is $0.9712$ as shown in Table~\ref{tab:postags}.
%using the given POS tags and accuracies from Table~\ref{tab:postags}, the increase in the weights would be respectively $0.9963+0.9712/2$, $0.998+0.9712/2$ and %$0.9971+0.9712/2$.

\begin{table}[hbt]
  \caption{Example edges extracted from the sample phrase  ``with a beautiful smile''}
  \centering
  \begin{tabular}[tc]{l}
 from : with$|$P, to : smile$|$N, dis : 2, weight : 89. \\
 from : a$|$D, to : smile$|$N, dis : 1, weight : 274. \\
 from : beautiful$|$A, to : smile$|$N, dis : 0, weight : 305. \\
\end{tabular}
\label{tab:edges}
\end{table}

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.5]{fig/edges}
\caption{Sample nodes and edges from the word association graph.}
\label{fig:edges}
\end{center}
\end{figure}


\section{Graph Based Contextual Similarity}

Our graph based contextual similarity method is based on the assumption that an IV word that is the canonical form of an OOV word appears in the same context with the corresponding OOV word. In other words, the two nodes in the graph share several neighbors that co-occur within the same distances to the corresponding two words in social media text. We also assume that an OOV word and its canonical form should have the same POS tag.

Given an input text for normalization, the next step after preprocessing is finding the normalization candidates for each OOV token in the input text. For each ill-formed OOV token $t_i$ in the input text, first the list of tokens that co-occur with $t_i$ in the input text and their positional distances to $t_i$ are extracted.  This list is called the neighbor list of token $t_i$, i.e.,  $NL(t_i)$. Table~\ref{tab:neigh} shows a sample neighbor list for the OOV token beatiful$|$A from the sample sentence in Table~\ref{tab:postagged}.

\begin{table}[hbt]
  \centering
  \begin{tabular}[tc]{l}
    id: w, tag: P, position: -2 \\
    id: a, tag: D, position: -1 \\
    id: smile, tag: V, position: 1 \\
  \end{tabular}
\caption{Example neighbor list for the OOV node beatiful$|$A}
\label{tab:neigh}
\end{table}

For each neighbor node $n_{j}$ in $NL$, the word association graph is traversed, and the edges from or to the node $n_{j}$ are extracted. The resulting edge list $EL(t)$ has edges in the form of ($n_{j}$, $c_{k}$) or ($c_{k}$, $n_{j}$), where $c_{k}$ is a candidate canonical form of the OOV word $t_i$.
Here the neighbor node $n_{j}$ can be an OOV node, but the candidate node $c_{k}$ is chosen among the IV nodes.
The edges in $EL(t)$ are filtered by the relative distance($p_{n_j}$) of $n_{j}$ to $t$ as given in the $NL(t)$. Any edge between  $n_{j}$ and $c_{k}$, whose distance is not the same as the distance between $n_{j}$ and $t$ is removed.

In addition to distance based filtering, POS tag based filtering is also performed on the edges in $EL(t)$. Each candidate node should have the same POS tag with the corresponding OOV token. For the OOV token $t_i$ that has the POS tag $T_i$, all the edges that include candidates with a tag other than $T_i$ are removed from the edge list $EL(t_{i})$. Thus, $EL(t_{i})$ only contains edges where candidate nodes are tagged as $T_i$, see Figure~\ref{fig:edgeWeight}.

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.5]{fig/edgeWeight}
\caption{$NL(beatiful) = with, a, smile$; $EL(beatiful)=(with,broken,edgeWeight=3),~(with,nice,edgeWeight=5)~...~(great,smile,edgeWeight=20)$}
\label{fig:edgeWeight}
\end{center}
\end{figure}

Each edge in $EL(t)$ consists of a neighbor node $n$, a candidate node $c$ and an edge weight $edgeWeight(n,c)$ between them. The edge weight, represents the likelihood or the strength of association between the neighbor node $n$ and the candidate node $c$ (Eq~\ref{eq:ew}). As described in the previous section the edge weights are computed based on the frequency of co-occurrence of two tokens, as well as the confidence scores of their POS tags.
%As shown in Eq~\ref{eq:ew}, we assume that the direction of the edge between two tokens doesn't affect the strength of association between them (i.e. the edge weight $w$).
Although this edge weight metric is reasonable for identifying the most likely canonical form for the OOV word $t_i$, it has the drawback of favoring words with high frequencies like the stop words. Therefore, we normalize the edge weight $edgeWeight(n,c)$ with the frequency of the candidate node $c$ as shown in $Eq~\ref{eq:ew_norm}$.

\begin{equation}
edgeWeight(n,c) =
\begin{cases}
  w : (n,c,distance = |p_n|,weight=w), & \text{if } p_n < 0 \\
  w : (c,n,distance = |p_n|,weight=w), & \text{otherwise}
\end{cases}
\label{eq:ew}
\end{equation}

\begin{equation}
ewNormalized(n,c) = edgeWeight(n,c) / frequency(c)
\label{eq:ew_norm}
\end{equation}

$Eq~\ref{eq:ew_norm}$ provides a metric that captures contextual similarity based on binary associations.
%However we need more than binary relatedness to achive a comprehensive contextual coverage.
In order to achieve a more comprehensive contextual coverage, a contextual similarity feature is built based on the sum of the binary association scores of several neighbors. As shown in Equation~\ref{eq:wscore1}, for a candidate node $c$ the total edge weight score is the sum of the normalized edge weight scores $ewNormalized(n,c)$, which are the edge weights coming from the different neighbors of the OOV token $t_i$. We expect this contextual similarity feature to favor and identify the candidates which are (1) related to many neighbors, and (2) have a high association score with each neighbor(i.e.~Figure\ref{fig:contextscores}).

\begin{equation}
edgeWeightScore(t,c) = \sum_{(n_j,c) or (c,n_j) \in EL(t)}{ewNormalized(n_j,c)} \\\\
\label{eq:wscore1}
\end{equation}

\begin{figure}[htb]
\begin{center}
\includegraphics[scale=0.5]{fig/contextScores}
\caption{Candidates for ``beatiful'' sorted by their edge weight scores}
\label{fig:contextscores}
\end{center}
\end{figure}

Our word association graph includes both OOV and IV tokens, and our OOV detection depends on the spellchecker which fails to identify some OOV tokens that have the same spelling with an IV word. In order to propose better canonical forms, the frequencies of the normalization candidates in the social media corpus have also been incorporated to the contextual similarity feature. Nodes with higher frequencies lead to tokens that are in their most likely grammatical forms.

The final contextual similarity of the token $t$ and the candidate $c$ is the weighted sum of the total edge weight score and the frequency score of the candidate (See Eq~\ref{eq:contscore}). The frequency score of the candidate is a real number between 0 and 1. It is proportional to the frequency of the candidate with respect to the frequencies of the other candidates in the corpus. Since the total edge weight score is our primary contextual resource, the weight of the frequency feature is set as half of the weight of the total edge weight score.

\begin{equation}
contSimScore(t,c) = \lambda_a edgeWeightScore(t,c) + \frac{\lambda_a} 2 freqScore(c)
\label{eq:contscore}
\end{equation}

Hereby, we have the candidate list $CL(t_{i})$ for the OOV token $t_i$ that includes all the unique candidates in $EL(t_{i})$ and their contextual similarity scores calculated.

\section{Lexical Similarity}

Following the prior work in~\cite{Han:2011:LNS:2002472.2002520,DBLP:conf/acl/HassanM13}, our lexical similarity features are based on edit distance~\cite{levenshtein1966bcc}, double metaphone (phonetic edit distance)~\cite{Philips:2000:DMS:349124.349132}, and longest common subsequence ratio (LCSR)~\cite{Contractor:2010:UCN:1944566.1944588}.

Following the tradition that is inspired from~\cite{Kaufmann2010} before lexical similarity calculations, any repetitions of characters three or more times in OOV tokens are reduced to two (e.g. \emph{goooood} is reduced to \emph{good}). Then, the edit distance, phonetic edit distance, and LCSR between each candidate in $CL(t_{i})$ and the OOV token $t_i$ are calculated. Edit distance and phonetic edit distance are used to filter the candidates. Any candidate in $CL(t_{i})$ with an edit distance greater than $ed_t$ and phonetic edit distance greater than $ped_t$ to $t_i$ has been removed from the candidate list $CL(t_{i})$.

For the remaining candidates, the total lexical similarity score (Eq~\ref{eq:lexscore}) is calculated using LCSR and edit distance score\footnote{an approximate string comparison measure (between 0.0 and 1.0) using the edit distance \url{https://sourceforge.net/projects/febrl/}}. Since the main lexical feature is LCSR, it is assigned twice the weight of the edit distance score. Since some social media text messages are extremely short and contain several OOV words, they do not provide sufficient context, i.e., IV neighbors, to enable the extraction of good candidates from the word association graph. Therefore, we extended the candidate list obtained through contextual similarity as described in the previous section, by including all the tokens in the word association graph that satisfy the edit distance and phonetic edit distance criteria. We also incorporated candidates from external resources, in other words from a slang dictionary and a transliteration table of numbers and pronouns (Table ~\ref{tab:transliteral}). If a token occurs in the slang dictionary or in the transliteration table it is assigned an external score of $1$, otherwise it is assigned an external score of $0$.

\begin{equation}
lexSimScore(t,c) = \lambda_a LCSR(t,c) + \frac{\lambda_a} 2 editDistScore(t,c) + \lambda_a externalScore(t,c)
\label{eq:lexscore}
\end{equation}

\begin{table}[ht]
  \begin{minipage}[c]{0.5\linewidth}
    \begin{tabular}[l]{lll}
    \hline
    token & tag & Transliteration \\
    \hline
    1 & ``\$'' & ``one'' \\
    2 & ``\$'' & ``two'' \\
    3 & ``\$'' & ``three'' \\
    4 & ``\$'' & ``for'' \\
    5 & ``\$'' & ``five'' \\
    6 & ``\$'' & ``six'' \\
    7 & ``\$'' & ``seven'' \\
    \hline
  \end{tabular}
\end{minipage}
  \begin{minipage}[c]{0.5\linewidth}
    \begin{tabular}[l]{lll}
    \hline
    token & tag & Transliteration \\
    \hline
    8 & ``\$'' & ``eight'' \\
    9 & ``\$'' & ``nine'' \\
    0 & ``\$'' & ``zero'' \\
    2 & ``P''  & ``to'' \\
    ``w'' & ``P''  & ``with'' \\
    ``im'' & ``L''  & ``I'm'' \\
    ``cont'' & ``\textasciitilde''  & ``continued'' \\
    \hline
  \end{tabular}
  \end{minipage}
  \caption{Transliteration Candidates improved~\cite{Gouws:2011:CBL:2021109.2021113}}
\label{tab:transliteral}
\end{table}

%\begin{verbatim}
%units = ["", "one", "to", "three", "for",
%         "five", "six", "seven", "eight", "nine"]
%pronouns = {u'2':u"to",u'w':u"with"}
%\end{verbatim}

As shown in Equation \ref{eq:candscore}, the final score of a candidate IV token $c$ for an OOV token $t$ is the sum of its lexical similarity score and contextual similarity score with respect to $t$.
\begin{equation}
candScore(t,c) = lexSimScore(t,c) + contSimScore(t,c)
\label{eq:candscore}
\end{equation}

\chapter{EXPERIMENTS}
\label{sec:experiments}

\section{Data sets}
We used the LexNorm1.1 dataset~\cite{Han:2011:LNS:2002472.2002520} and Pennell et al.'s trigram dataset~\cite{pennell2011character} to evaluate our proposed approach. LexNorm1.1 contains $549$ tweets with $1184$ manually annotated ill-formed OOV tokens. It has been used by recent text normalization studies for evaluation, which enables us to directly compare our performance results with results obtained by the recent previous work. Trigram dataset on the other hand is an SMS-like corpus collected from twitter status updates sent via SMS. The dataset does not include the complete tweet text but trigrams from tweets and one OOV word in each trigram is annotated. In total 4661 twitter status messages and 7769 tokens are annotated, previous work has used 80 percent for training and the rest as test set, similarly we used the same 20 percent of the dataset as test set to be able to report on the same basis as previous work.

\section{Graph Generation}
We used a large corpus of social media text to construct our word association graph. We extracted 1.5 GB of English tweets from Stanford's 476 million Twitter Dataset~\cite{DBLP:conf/wsdm/YangL11}. The language identification of tweets was performed by using the langid.py Python library~\cite{Lui:2012:LOL:2390470.2390475, Baldwin:2010:LIL:1857999.1858026}.

CMU Ark Tagger, which is a social media specific POS tagger achieving an accuracy of $95\%$ over social media text ~\cite{owoputi2013improved,Gimpel:2011:PTT:2002736.2002747}, is used for tokenizing and POS tagging the tweets. Besides the standard POS tags, the POS tagset of the Ark Tagger includes some extra POS tags specific to social media including URLs and emoticons; Twitter hashtags \#; and twitter at-mentions (@). One other tag that is special to social media is \textasciitilde~ that means the token is specific to a discourse function of twitter. Lastly G stands for miscellaneous words including multi word abbreviations like btw~(by the way), nw~(no way), and smh~(somehow).

We made use of these social media specific tags to disambiguate some OOV tokens. For example if OOV token ``cont'' is tagged with the discourse function tag G, we added ``continued'' to the candidate list as an external node.

After tokenization, we removed the tokens that were POS tagged as mention~(e.g. @brendon), discourse marker (e.g. RT), URL, email address, emoticon, numeral and punctuation. The remaining tokens are used to build the word association graph. After constructing the graph we only kept the nodes with a frequency greater than $8$. For the performance related reasons, the relatedness thresholds $d_t$ and $f_t$  were chosen as $3$ and $8$, respectively. The resulting graph contains $105428$ nodes and $46609603$ edges.

\section{Candidate Set Generation}

While extending the candidate set with lexical features we use ${ed_t \leq 2} \vee {ped_t \leq 1}$ to keep up with the settings in Han et al.~\cite{Han:2011:LNS:2002472.2002520}. In other words, IV words that are within 2 character edit distance of a given OOV word or 1 character edit distance of a given OOV word under phonemic transcription were chosen as lexical similarity candidates.

\chapter{RESULTS AND ANALYSIS}

The results obtained by our proposed Contextual Word Association Graph (CWA-Graph) system on the LexNorm1.1 dataset,
as well as the results of recent studies that used the same data set for evaluation are presented in Table~\ref{tab:results}.

\begin{table}[thb]
  \caption{Results obtained on the LexNorm1.1 dataset.}
  \centering
  \begin{tabular}[t]{lccc}
    \hline
    Method & Precision & Recall & F-measure \\
    \hline
    Han and Baldwin, 2011 & 75.30 & 75.30 & 75.30 \\
    Liu et al., 2011 & 84.13 & 78.38 & 81.15 \\
    Hassan et al., 2013 & 85.37 & 56.40 & 69.93 \\
    Yang et al., 2013 & 82.09 & 82.09 & 82.09 \\
    CWA-Graph   & 85.50 & 79.20 & 82.20 \\
    \hline
  \end{tabular}
  \label{tab:results}
\end{table}

Our CWA-Graph approach achieves the best precision ($86.20$) among the recent previous studies. The high precision value is obtained without compromising much from recall ($78.0$). The F-score of the CWA-Graph system ($82.0$) is very close to the state-of-the-art F-score ($82.09$) obtained by Yang et al.'s system, which on the other hand, has a lower precision than our approach~\cite{DBLP:conf/emnlp/YangE13}.

The earlier work we compare our system with, assumes that the words to be normalized are given in advance. We also made the same assumption. However unlike other systems (\cite{DBLP:conf/emnlp/YangE13,liu2012broad,Han:2011:LNS:2002472.2002520}), our system may not propose a normalization, if there are no candidates that are lexically similar, grammatically correct and contextually close enough. For this reason we managed to achieve a higher precision compared to the other systems. Besides, we made sure that the candidates have a minimum similarity either contextual, lexical, external or some degree of each feature. Table~\ref{tab:thresholds} shows that our approach can obtain even higher values of precision by tuning the system threshold (i.e.~the minimum score in Equation~\ref{eq:candscore} to return a token as a candidate canonical form of an OVV token).

\begin{table}[thb]
  \caption{Comparison of results for different threshold values}
  \centering
  \begin{tabular}[th]{lccc}
    \hline
    Threshold & Precision & Recall & F-measure \\
    \hline
    $\leq$ 1 & 81.2 &	80.8 &	81 \\
    1.1 & 81.5 & 80.8 &	81.2 \\
    1.2 & 82.2 &	80.7 &	81.4 \\
    1.3 & 83.7 &	80.2 &	81.9 \\
    1.4 & 84.2 &	80.0 &	82.0 \\
    1.5 & \textbf{85.5} &  \textbf{79.2} &  \textbf{82.2} \\
    1.6 & 88.8	& 75.1	& 81.4 \\
    \hline
  \end{tabular}
\label{tab:thresholds}
\end{table}

We also test our system on different window sizes. As shown in Table~\ref{tab:windows}, the system achives best results with a window size 7. With higher window sizes OOV tokens get context information from outer word phrases which includes contextually less relevant words. Also working with small window sizes results in the lack of an important amount of contextual information. Considering twitter's limit on message lenght and users' tendency of using url and long hastags but short texts, the window size is not very large as expected.

\begin{table}[thb]
  \caption{Comparison of results for different window sizes}
  \centering
  \begin{tabular}[th]{lccc}
    \hline
    Window size & Precision & Recall & F-measure \\
    \hline
    3 & 85.3 & 79.0 & 82.0 \\
    5 & 85.6 & 79.1 & 82.2 \\
    7 & \textbf{85.5} &  \textbf{79.2} &  \textbf{82.2} \\
    9 & 85.2 & 79.0  & 82.0 \\
    \hline
  \end{tabular}
\label{tab:windows}
\end{table}

Our system performed quite good on the trigram SMS-like dataset without further changes, we achieved to improve Pennell et. al.'s MT system~\cite{pennell2011character}. You can find the results presented in Table~\ref{tab:resultspennell}.

\begin{table}[thb]
  \caption{Results obtained on the trigram SMS-like dataset.}
  \centering
  \begin{tabular}[t]{lccc}
    \hline
    Method & Precision & Recall & F-measure \\
    \hline
    Pennell et al., 2011 & 69.70 & 69.70 & 69.70 \\
    CWA-Graph   & 71.70 & 68.80 & 70.30 \\
    \hline
  \end{tabular}
  \label{tab:resultspennell}
\end{table}

\chapter{CONCLUSION}

We presented an unsupervised graph based approach for contextual text normalization. We compared our approach with the recent social media text normalization systems and achieved state-of-the-art precision and F-measure scores.

The proposed approach can analyze grammatical and contextual information from the noisy input text. The task of normalization is highly dependent on understanding and capturing the dynamics of informal nature of noisy text. Our word association graph is built using a large unlabeled social media corpus. It helps to derive contextual and grammatical analysis on both clean and noisy data.

Except for the double metaphone algorithm that encodes the phonetic similarities among words in English, the proposed approach is highly language independent. As future work, we will apply our system to different languages.
% \nocite{NewEntry1,NewEntry2,NewEntry3,NewEntry4,NewEntry5,
% NewEntry6,
% NewEntry7,NewEntry8,NewEntry9,NewEntry10,NewEntry11,NewEntry12}
%\appendix
%\chapter{APPLICATION}
%The appendices start here.
%\cite{*}
\bibliographystyle{styles/fbe_tez_v11}
\bibliography{sentic}

\end{document}


\begin{table}[tbhp]
\caption{Sample tokenized, POS tagged sentence and the corresponding nodes and edges in the word association graph.(L:nominal+verbal, V:verb, D:determiner, N:noun, A:adjective, C:punctuation)}
\begin{minipage}[c]{\linewidth}
\fxbox{
Let's$_{\textcolor{red}{L}}$ start$_{\textcolor{red}V}$ this$_{\textcolor{red}D}$ morning$_{\textcolor{red}N}$ w$_{\textcolor{red}P}$ a$_{\textcolor{red}D}$ beatiful$_{\textcolor{red}A}$ smile$_{\textcolor{red}N}$.$_{\textcolor{red}C}$
 }\par
\vspace{5mm}
\end{minipage}
\begin{minipage}[c]{\linewidth}
\begin{tabular}[h]{l|l}
Tokens & Let's, start, this, morning, w, a, beatiful, smile,~. \\
\hline
Nodes & Let's$|$L, start$|$V, this$|$D, morning$|$N, w$|$P, a$|$D, beatiful$|$A, smile$|$N, ,$|$C \\
\hline
Edges & \{Let's$|$L, start$|$V , distance:0\},\{Let's$|$L, this$|$D, distance:1\}, \\
& ... \\
& \{a$|$D, beatiful$|$A, distance:0\}, \{a$|$D, smile$|$N, distance:1\}, \\
& \{beatiful$|$A, smile$|$N, distance:0\} \\
\end{tabular}
\end{minipage}
\label{tab:graph}
\end{table}
